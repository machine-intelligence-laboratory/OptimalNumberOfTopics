{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopicBank: Bank Creation Experiment\n",
    "\n",
    "Here we are going to collect interpretable topics (automatically, using topic coherence) from multiple model training.\n",
    "These topics constitute *topic bank*.\n",
    "And then the topic bank is going to be used for estimating topic models quality in the notebook [TopicBank-Experiment: Model Validation](TopicBank-Experiment-ModelValidation.ipynb).\n",
    "\n",
    "The process is repeated for several datasets (some of them are already downloadable using [TopicNet](https://github.com/machine-intelligence-laboratory/TopicNet) library)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents<a id=\"contents\"></a>\n",
    "\n",
    "* [Data](#data)\n",
    "    * [Coocs](#coocs)\n",
    "        * [Lower Memory Consumption (or a Bit of Shamanism. Part 1)](#optimizing-memory)\n",
    "    * [Documents for Coherence Scores](#docs-for-cohs)\n",
    "        * [Lower Time Consumption in Case of Big Datasets](#optimizing-time)\n",
    "* [Experiment](#experiment)\n",
    "    * [Scores](#scores)\n",
    "    * [Bank Creation](#bank-creation)\n",
    "* [Postprocessing](#postprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "\n",
    "import dill\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from enum import Enum\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import (\n",
    "    Dict,\n",
    "    Iterable,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making `topnum` module visible for Python\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal number of topics\n",
    "\n",
    "from topicnet.cooking_machine import Dataset\n",
    "\n",
    "from topnum.data.vowpal_wabbit_text_collection import VowpalWabbitTextCollection\n",
    "from topnum.scores import (\n",
    "    IntratextCoherenceScore,\n",
    "    SparsityPhiScore,\n",
    "    SparsityThetaScore,\n",
    "    SimpleTopTokensCoherenceScore,\n",
    "    SophisticatedTopTokensCoherenceScore,\n",
    ")\n",
    "from topnum.scores._base_coherence_score import (\n",
    "    SpecificityEstimationMethod,\n",
    "    TextType,\n",
    "    WordTopicRelatednessType,\n",
    ")\n",
    "from topnum.scores.intratext_coherence_score import ComputationMethod\n",
    "from topnum.search_methods import TopicBankMethod\n",
    "from topnum.search_methods.topic_bank.topic_bank import TopicBank\n",
    "from topnum.search_methods.topic_bank.one_model_train_funcs import (\n",
    "    default_train_func,\n",
    "\n",
    "    # Functions below are not used (but could have been)\n",
    "\n",
    "#     regularization_train_func,\n",
    "#     specific_initial_phi_train_func,\n",
    "#     background_topics_train_func,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data<a id=\"data\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Loading data from disk, creating batches, dictionary, gathering cooccurrence statistics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH = os.path.join('.', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '20NG_natural_order.csv',\n",
       " '20NG_natural_order_250.csv',\n",
       " '20NG_natural_order_40.csv',\n",
       " '20NG_natural_order_45.csv',\n",
       " '20NG_natural_order_50.csv',\n",
       " '20NG_natural_order_55.csv',\n",
       " '20NG_natural_order_60.csv',\n",
       " '20NG_natural_order_65.csv',\n",
       " 'AG_News.csv',\n",
       " 'AG_News_100.csv',\n",
       " 'AG_News_1000.csv',\n",
       " 'AG_News_10000.csv',\n",
       " 'AG_News_250.csv',\n",
       " 'AG_News_500.csv',\n",
       " 'Brown.csv',\n",
       " 'Brown_10.csv',\n",
       " 'Brown_8.csv',\n",
       " 'Brown_80.csv',\n",
       " 'Brown_9.csv',\n",
       " 'Habrahabr.csv',\n",
       " 'Habrahabr_100.csv',\n",
       " 'Habrahabr_15.csv',\n",
       " 'Habrahabr_50.csv',\n",
       " 'PScience.csv',\n",
       " 'PostNauka_natural_order.csv',\n",
       " 'Post_Science',\n",
       " 'Post_Science_12.csv',\n",
       " 'Post_Science_15.csv',\n",
       " 'Post_Science_18.csv',\n",
       " 'Post_Science_20.csv',\n",
       " 'Post_Science_65.csv',\n",
       " 'Reuters.csv',\n",
       " 'Reuters_20.csv',\n",
       " 'Reuters_50.csv',\n",
       " 'Reuters_60.csv',\n",
       " 'Reuters_80.csv',\n",
       " 'Watan2004.csv',\n",
       " 'Watan2004_10.csv',\n",
       " 'Watan2004_100.csv',\n",
       " 'Watan2004_12.csv',\n",
       " 'Watan2004_15.csv',\n",
       " 'Watan2004_250.csv']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(DATA_FOLDER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetName(Enum):\n",
    "    POSTNAUKA = 'Post_Science'\n",
    "    REUTERS = 'Reuters'\n",
    "    BROWN = 'Brown'\n",
    "    TWENTY_NEWSGROUPS = '20NG_natural_order'\n",
    "    AG_NEWS = 'AG_News'\n",
    "    WATAN = 'Watan2004'\n",
    "    HABRAHABR = 'Habrahabr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME_TO_DATASET_FILE_PATH = {\n",
    "    DatasetName.POSTNAUKA: os.path.join(\n",
    "        DATA_FOLDER_PATH, 'PostNauka_natural_order.csv'\n",
    "    ),\n",
    "    DatasetName.REUTERS: os.path.join(\n",
    "        DATA_FOLDER_PATH, 'Reuters.csv'\n",
    "    ),\n",
    "    DatasetName.BROWN: os.path.join(\n",
    "        DATA_FOLDER_PATH, 'Brown.csv'\n",
    "    ),\n",
    "    DatasetName.TWENTY_NEWSGROUPS: os.path.join(\n",
    "        DATA_FOLDER_PATH, '20NG_natural_order.csv'\n",
    "    ),\n",
    "    DatasetName.AG_NEWS: os.path.join(\n",
    "        DATA_FOLDER_PATH, 'AG_News.csv'\n",
    "    ),\n",
    "    DatasetName.WATAN: os.path.join(\n",
    "        DATA_FOLDER_PATH, 'Watan2004.csv'\n",
    "    ),\n",
    "    DatasetName.HABRAHABR: os.path.join(\n",
    "        DATA_FOLDER_PATH, 'Habrahabr.csv'\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = DatasetName.POSTNAUKA  # select a dataset here\n",
    "\n",
    "DATASET_FILE_PATH = DATASET_NAME_TO_DATASET_FILE_PATH[DATASET_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if all OK with data, what modalities does the collection have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,raw_text,vw_text\r\n",
      "29998.txt,материал отрицательный показатель преломление физик виктор веселаго распространение свет вещество фазовый групповой скорость метаматериалы различаться фазовый групповой скорость каков физика распространение свет вещество находить применение материал отрицательный показатель преломление рассказывать доктор физикоматематический наука виктор веселаго скорость распространяться энергия вещество обычно говорить излучение распространяться вещество со скорость n раз маленький n коэффициент преломление вещество коэффициент преломление n отношение скорость свет скорость распространение излучение вещество обычно уточняться распространяться распространение энергия распространение импульс происходить различный закон энергия распространяться со скорость называться групповой скорость много скорость свет эйнштейн сформулировать самый больший скорость излучение скорость свет кмс импульс распространяться фазовый скорость сколь угодно много скорость свет скорость входить соотношение emc фазовый групповой оказываться здесь величина заменять произведение фазовый скорость групповой post id отрицательный преломление свет притягивать объект излучатель внутри вещество возможный явление чемто новый впервые указывать статья оказываться материал обладать отрицательный преломление описываться отрицательный коэффициент преломление n отношение скорость свет пустота скорость свет вещество излучатель освещать приемник внутри вещество приемник притягиваться излучатель отрицательный скорость положительный скорость излучатель приемник наоборот отрицательный классический школьный опыт брать вода пускать луч свет преломляться внутри вода идти угол идти поверхность вместо вода использовать вещество отрицательный преломление отклоняться налево направо комбинировать вещество отрицательный положительный преломление организовывать прохождение луч свет через набор материальный среда определенный закон както повертываться извиваться обычно привязывать эффект шапкиневидимка иметься эксперимент иметься некий объект сделать невидимый свет проходить вокруг объект наблюдатель кажется объект хотя дело практический применение далеко кроме явление связывать отрицательный преломление наблюдаться область электромагнитный излучение акустика распространение разный род колебание,29998.txt |@word материал отрицательный показатель преломление физик виктор веселаго распространение свет вещество фазовый групповой скорость метаматериалы различаться фазовый групповой скорость каков физика распространение свет вещество находить применение материал отрицательный показатель преломление рассказывать доктор физикоматематический наука виктор веселаго скорость распространяться энергия вещество обычно говорить излучение распространяться вещество со скорость n раз маленький n коэффициент преломление вещество коэффициент преломление n отношение скорость свет скорость распространение излучение вещество обычно уточняться распространяться распространение энергия распространение импульс происходить различный закон энергия распространяться со скорость называться групповой скорость много скорость свет эйнштейн сформулировать самый больший скорость излучение скорость свет кмс импульс распространяться фазовый скорость сколь угодно много скорость свет скорость входить соотношение emc фазовый групповой оказываться здесь величина заменять произведение фазовый скорость групповой post id отрицательный преломление свет притягивать объект излучатель внутри вещество возможный явление чемто новый впервые указывать статья оказываться материал обладать отрицательный преломление описываться отрицательный коэффициент преломление n отношение скорость свет пустота скорость свет вещество излучатель освещать приемник внутри вещество приемник притягиваться излучатель отрицательный скорость положительный скорость излучатель приемник наоборот отрицательный классический школьный опыт брать вода пускать луч свет преломляться внутри вода идти угол идти поверхность вместо вода использовать вещество отрицательный преломление отклоняться налево направо комбинировать вещество отрицательный положительный преломление организовывать прохождение луч свет через набор материальный среда определенный закон както повертываться извиваться обычно привязывать эффект шапкиневидимка иметься эксперимент иметься некий объект сделать невидимый свет проходить вокруг объект наблюдатель кажется объект хотя дело практический применение далеко кроме явление связывать отрицательный преломление наблюдаться область электромагнитный излучение акустика распространение разный род колебание\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 $DATASET_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_internals_folder_path(dataset_name: DatasetName) -> str:\n",
    "    return os.path.join('.', dataset_name.value + '__internals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_INTERNALS_FOLDER_PATH = get_dataset_internals_folder_path(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_INTERNALS_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# If using really big datasets (like Habrahabr),\n",
    "# one may need to set this equal `False`\n",
    "KEEP_DATASET_IN_MEMORY = True\n",
    "\n",
    "DATASET = Dataset(\n",
    "    DATASET_FILE_PATH,\n",
    "    internals_folder_path=DATASET_INTERNALS_FOLDER_PATH,\n",
    "    keep_in_memory=KEEP_DATASET_IN_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking what is inside dataset's folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vw.txt',\n",
       " 'vocab.txt',\n",
       " 'dict.dict.txt',\n",
       " 'new_ppmi_tf_',\n",
       " 'ppmi_tf_',\n",
       " 'dict.dict',\n",
       " 'cooc_values.json',\n",
       " 'batches']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATASET_INTERNALS_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artm.BatchVectorizer(data_path=\"./Post_Science__internals__test/batches\", num_batches=4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET.get_batch_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vw.txt',\n",
       " 'vocab.txt',\n",
       " 'dict.dict.txt',\n",
       " 'new_ppmi_tf_',\n",
       " 'ppmi_tf_',\n",
       " 'dict.dict',\n",
       " 'cooc_values.json',\n",
       " 'batches']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATASET_INTERNALS_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 3446\n"
     ]
    }
   ],
   "source": [
    "if KEEP_DATASET_IN_MEMORY:\n",
    "    DOCUMENTS = list(DATASET._data.index)\n",
    "else:\n",
    "    DOCUMENTS = list(DATASET._data_index)\n",
    "\n",
    "NUM_DOCUMENTS = len(DOCUMENTS)\n",
    "\n",
    "print(f'Num documents: {NUM_DOCUMENTS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>vw_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29998.txt</th>\n",
       "      <td>29998.txt</td>\n",
       "      <td>материал отрицательный показатель преломление ...</td>\n",
       "      <td>29998.txt |@word материал отрицательный показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7770.txt</th>\n",
       "      <td>7770.txt</td>\n",
       "      <td>культурный код экономика экономист александр а...</td>\n",
       "      <td>7770.txt |@word культурный код экономика эконо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32230.txt</th>\n",
       "      <td>32230.txt</td>\n",
       "      <td>faq наука третий класс факт эксперимент резуль...</td>\n",
       "      <td>32230.txt |@word faq наука третий класс факт э...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27293.txt</th>\n",
       "      <td>27293.txt</td>\n",
       "      <td>обрушение волна поверхность жидкость математик...</td>\n",
       "      <td>27293.txt |@word обрушение волна поверхность ж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481.txt</th>\n",
       "      <td>481.txt</td>\n",
       "      <td>существовать ли суперсимметрия мир элементарны...</td>\n",
       "      <td>481.txt |@word существовать ли суперсимметрия ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                           raw_text  \\\n",
       "id                                                                        \n",
       "29998.txt  29998.txt  материал отрицательный показатель преломление ...   \n",
       "7770.txt    7770.txt  культурный код экономика экономист александр а...   \n",
       "32230.txt  32230.txt  faq наука третий класс факт эксперимент резуль...   \n",
       "27293.txt  27293.txt  обрушение волна поверхность жидкость математик...   \n",
       "481.txt      481.txt  существовать ли суперсимметрия мир элементарны...   \n",
       "\n",
       "                                                     vw_text  \n",
       "id                                                            \n",
       "29998.txt  29998.txt |@word материал отрицательный показа...  \n",
       "7770.txt   7770.txt |@word культурный код экономика эконо...  \n",
       "32230.txt  32230.txt |@word faq наука третий класс факт э...  \n",
       "27293.txt  27293.txt |@word обрушение волна поверхность ж...  \n",
       "481.txt    481.txt |@word существовать ли суперсимметрия ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET._data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coocs<a id=\"coocs\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Below we will make `vocab.txt` for computing word cooccurrences and compute coocs themselves.\n",
    "Cooccurrences are needed for some coherence scores.\n",
    "\n",
    "P.S.\n",
    "The notebook [Making-Decorrelation-and-Topic-Selection-Friends.ipynb](https://github.com/machine-intelligence-laboratory/TopicNet/blob/master/topicnet/demos/Making-Decorrelation-and-Topic-Selection-Friends.ipynb) contains a bit more explanation and references concerning cooccurrences computation in ARTM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/dict.dict'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET._dictionary_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need dictionary saved in human readable text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_TXT_FILE_PATH = DATASET._dictionary_file_path + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals/dict.dict.txt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICTIONARY_TXT_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = DATASET.get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artm.Dictionary(name=3ac8eb64-8702-4d53-8cab-ed55e4f9a715, num_entries=82162)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do dictionary filtering now so as to remove too frequent words (and compute coocs only for meaningful words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artm.Dictionary(name=3ac8eb64-8702-4d53-8cab-ed55e4f9a715, num_entries=5112)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter(min_df_rate=0.01, max_df_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving dictionary as text file and inside dataset entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_text(DICTIONARY_TXT_FILE_PATH)\n",
    "\n",
    "DATASET._cached_dict = dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read dictionary entries to make a `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_lines = open(DICTIONARY_TXT_FILE_PATH, 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5114"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name: 3ac8eb64-8702-4d53-8cab-ed55e4f9a715 num_items: 3446\\n',\n",
       " 'token, class_id, token_value, token_tf, token_df\\n',\n",
       " 'социокультурный, @word, 3.8952195609454066e-05, 87.0, 39.0\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['легкий, @word, 8.909754978958517e-05, 199.0, 149.0\\n',\n",
       " 'строительный, @word, 2.999766729772091e-05, 67.0, 49.0\\n',\n",
       " 'внизу, @word, 1.8804508727043867e-05, 42.0, 36.0\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_lines[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text = ''\n",
    "\n",
    "for line in dictionary_lines[2:]:\n",
    "    token, modality, _, _, _ = line.strip().split(', ')\n",
    "    vocab_text += f'{token} {modality}\\n'\n",
    "\n",
    "vocab_file_path = os.path.join(\n",
    "    DATASET_INTERNALS_FOLDER_PATH,\n",
    "    'vocab.txt',\n",
    ")\n",
    "\n",
    "with open(vocab_file_path, 'w') as f:\n",
    "    f.write(vocab_text)\n",
    "\n",
    "\n",
    "del vocab_text\n",
    "del dictionary_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one must run BigARTM Command Line Utility to gather cooccerrences statistics.\n",
    "To do so, there should be BigARTM executable file on the machine!\n",
    "If `topicnet` was, for example, installed via `pip`, there may be no such ARTM executable.\n",
    "However, one need exactly this one to run the command below.\n",
    "There are some references abot BigARTM CLI here on [TopicNet's GitHub page](https://github.com/machine-intelligence-laboratory/TopicNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path and run this cell, or run the command in bash\n",
    "\n",
    "! ~/<YOUR PATH>/bigartm/build/bin/bigartm \\\n",
    "    -c $DATASET_INTERNALS_FOLDER_PATH/vw.txt \\\n",
    "    -v vocab.txt \\\n",
    "    --cooc-window 10 \\\n",
    "    --cooc-min-tf 2 \\\n",
    "    --write-cooc-tf cooc_tf_ \\\n",
    "    --cooc-min-df 2 \\\n",
    "    --write-cooc-df cooc_df_ \\\n",
    "    --write-ppmi-tf ppmi_tf_ \\\n",
    "    --write-ppmi-df ppmi_df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is supposed to transform `ppmi_tf_` or `ppmi_df_` contents in a format used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coocs_file(source_file_path, target_file_path, vocab_file_path):\n",
    "    \"\"\"\n",
    "    Source file is assumed to be either `ppmi_tf_` or `ppmi_df_`\n",
    "\n",
    "    \"\"\"\n",
    "    num_times_to_log = 10\n",
    "\n",
    "    vocab = open(vocab_file_path, 'r').readlines()\n",
    "    vocab = [l.strip().split()[0] for l in vocab]\n",
    "    \n",
    "    cooc_values = dict()\n",
    "    word_word_value_triples = set()\n",
    "    \n",
    "    lines = open(source_file_path, 'r').readlines()\n",
    "    \n",
    "    for i, l in enumerate(lines):\n",
    "        if i % (len(lines) // num_times_to_log) == 0:\n",
    "            print(f'{i:6d} lines out of {len(lines)}')\n",
    "        \n",
    "        l = l.strip()\n",
    "        words = l.split()\n",
    "\n",
    "        if words[0].startswith('@'):  # modality\n",
    "                                      # (not always included:\n",
    "                                      #  @default_class seems not used in vocab.txt)\n",
    "            words = words[1:]         # excluding modality\n",
    "        \n",
    "        anchor_word = words[0]\n",
    "        other_word_values = words[1:]\n",
    "        \n",
    "        for word_and_value in other_word_values:\n",
    "            other_word, value = word_and_value.split(':')\n",
    "            value = float(value)\n",
    "            \n",
    "            cooc_values[(anchor_word, other_word)] = value\n",
    "            \n",
    "            # No need to do so: coherence scores do similar thing under the hood\n",
    "            # cooc_values[(other_word, anchor_word)] = value  # if assume cooc values to be symmetric\n",
    "            \n",
    "            word_word_value_triples.add(\n",
    "                tuple([\n",
    "                    tuple(sorted([\n",
    "                        vocab.index(anchor_word),\n",
    "                        vocab.index(other_word)\n",
    "                    ])),\n",
    "                    value\n",
    "                ])\n",
    "            )\n",
    "    \n",
    "    new_text = ''\n",
    "    \n",
    "    for (w1, w2), v in word_word_value_triples:\n",
    "        new_text += f'{w1} {w2} {v}\\n'\n",
    "    \n",
    "    with open(target_file_path, 'w') as f:\n",
    "        f.write(''.join(new_text))\n",
    "    \n",
    "    return cooc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 lines out of 1673\n",
      "   167 lines out of 1673\n",
      "   334 lines out of 1673\n",
      "   501 lines out of 1673\n",
      "   668 lines out of 1673\n",
      "   835 lines out of 1673\n",
      "  1002 lines out of 1673\n",
      "  1169 lines out of 1673\n",
      "  1336 lines out of 1673\n",
      "  1503 lines out of 1673\n",
      "  1670 lines out of 1673\n"
     ]
    }
   ],
   "source": [
    "COOC_VALUES = transform_coocs_file(\n",
    "    os.path.join(DATASET_INTERNALS_FOLDER_PATH, 'ppmi_tf_'),\n",
    "    os.path.join(DATASET_INTERNALS_FOLDER_PATH, 'new_ppmi_tf_'),\n",
    "    os.path.join(DATASET_INTERNALS_FOLDER_PATH, 'vocab.txt'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465882"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(COOC_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('както', 'понимать'), 0.697806) (('както', 'сталкиваться'), 0.388971)\n"
     ]
    }
   ],
   "source": [
    "it = iter(COOC_VALUES.items())\n",
    "\n",
    "print(next(it), next(it))\n",
    "\n",
    "del it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are too many coocs (more than $1\\,000\\,000$ let's say) — better do some additional filtering.\n",
    "For example, keep only coocs with numeric value bigger than some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.67684e-07"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(COOC_VALUES.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.46194"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(COOC_VALUES.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_cooc = np.percentile(list(COOC_VALUES.values()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08462922"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_cooc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOC_VALUES = {\n",
    "    k: v for i, (k, v) in enumerate(COOC_VALUES.items()) if v >= threshold_cooc\n",
    "}\n",
    "\n",
    "del threshold_cooc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419293"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(COOC_VALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving coocs on disk (in case notebook crashes, for example) and than loading them to check if all OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOC_VALUES_FILE_PATH = os.path.join(\n",
    "    DATASET_INTERNALS_FOLDER_PATH, 'cooc_values.json'\n",
    ")\n",
    "\n",
    "with open(COOC_VALUES_FILE_PATH, 'w') as f:\n",
    "    f.write(json.dumps(list(COOC_VALUES.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419293"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(COOC_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del COOC_VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['както', 'понимать'], 0.697806], [['както', 'сталкиваться'], 0.388971], [['както', 'переводить'], 0.543726], [['както', 'любовь'], 0.905235], [['както', 'возможно'], 0.342641], [['както', 'экономист'], 0.119018], [['както', 'дорога'], 0.537558], [['както', 'академический'], 0.357577], [['както', 'проявлять'], 0.827184], [['както', 'проблема'], 0.464159], [['както', 'подавлять'], 0.581892], [['както', 'трудно'], 0.735254], [['както', 'действительно'], 0.263179], [['както', 'оставаться'], 0.162784], [['както', 'образовываться'], 0.712785], [['както', 'молодой'], 0.623452], [['както', 'пора'], 0.332543], [['както', 'очевидно'], 0.415827], [['както', 'насколько'], 0.429967], [['както', 'неделя'], 1.35179]]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(COOC_VALUES_FILE_PATH):\n",
    "    COOC_VALUES = dict()\n",
    "else:\n",
    "    raw_cooc_values = json.loads(open(COOC_VALUES_FILE_PATH, 'r').read())\n",
    "    \n",
    "    print(\n",
    "        raw_cooc_values[:20]\n",
    "    )\n",
    "\n",
    "    COOC_VALUES = {\n",
    "        tuple(d[0]): d[1] for d in raw_cooc_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419293"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(COOC_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_COOC_KEY = list(COOC_VALUES.keys())[419126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('прекрасно', 'огромный')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_COOC_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Memory Consumption (or a Bit of Shamanism. Part 1)<a id=\"optimizing-memory\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "It so happened that coocs are not stored in the best way possible in the coherence scores in the module.\n",
    "Let's try to fix it right here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.126559"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COOC_VALUES.__getitem__(SAMPLE_COOC_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting words in a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419293/419293 [00:00<00:00, 2343319.19it/s]\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY = set()\n",
    "\n",
    "for word_a, word_b in tqdm(COOC_VALUES.keys(), file=sys.stdout):\n",
    "    VOCABULARY.add(word_a)\n",
    "    VOCABULARY.add(word_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VOCABULARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_TO_INDEX = {\n",
    "    w: i for i, w in enumerate(VOCABULARY)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WORD_TO_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a trick: we imitate coocs values, but using word indices instead of words.\n",
    "So, we need to store only vocabulary + index pairs instead of word pairs.\n",
    "If there are many coocs (more than $1\\,000\\,000$), this may help to optimize the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoocValues:\n",
    "    def __init__(self, word2index, coocs):\n",
    "        self._word2index = word2index\n",
    "        self._coocs = coocs\n",
    "        \n",
    "        self._index2word = {\n",
    "            i: w\n",
    "            for w, i in self._word2index.items()\n",
    "        }\n",
    "    \n",
    "    def _map_key(self, word_pair_key):\n",
    "        word_a, word_b = word_pair_key\n",
    "        index_a = self._word2index[word_a]\n",
    "        index_b = self._word2index[word_b]\n",
    "\n",
    "        return (index_a, index_b)\n",
    "\n",
    "    def __getitem__(self, word_pair_key):\n",
    "        return self._coocs[self._map_key(word_pair_key)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._coocs)\n",
    "    \n",
    "    def has_key(self, word_pair_key):\n",
    "        word_a, word_b = word_pair_key\n",
    "\n",
    "        return (\n",
    "            word_a in self._word2index and\n",
    "            word_b in self._word2index and\n",
    "            self._map_key(word_pair_key) in self._coocs\n",
    "        )\n",
    "    \n",
    "    def __contains__(self, k):\n",
    "        return self.has_key(k)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for (index_a, index_b) in self._coocs:\n",
    "            yield (\n",
    "                self._index2word[index_a],\n",
    "                self._index2word[index_b]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOC_VALUES = CoocValues(WORD_TO_INDEX, _COOC_VALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if all OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.126559"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COOC_VALUES[SAMPLE_COOC_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419293"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(COOC_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('както', 'понимать')\n"
     ]
    }
   ],
   "source": [
    "for c in COOC_VALUES:\n",
    "    print(c)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del SAMPLE_COOC_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dpcuments for Coherence Scores<a id=\"docs-for-cohs\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Now we need tp choose documents to be used by coherence scores.\n",
    "Choosing just all documents is not a good choice, because things will be too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing documents lengths (to select only not so small and not so big documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = DATASET._data['vw_text'].apply(lambda text: len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.63 s, sys: 0 ns, total: 2.63 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not KEEP_DATASET_IN_MEMORY:\n",
    "    lengths = lengths.compute()\n",
    "    lengths = lengths.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not KEEP_DATASET_IN_MEMORY:\n",
    "    DATASET.get_vw_document(DATASET._data_index[0])\n",
    "else:\n",
    "    DATASET.get_vw_document(DATASET._data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.00 (25%) < 462.50 (median) < 871.75 (75%)\n"
     ]
    }
   ],
   "source": [
    "median_length = np.median(lengths)\n",
    "p25_length = np.percentile(lengths, 25)\n",
    "p75_length = np.percentile(lengths, 75)\n",
    "\n",
    "print(f'{p25_length:.2f} (25%) < {median_length:.2f} (median) < {p75_length:.2f} (75%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These documents are of ordinary lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDINARY_DOCUMENTS = [\n",
    "    d for i, d in enumerate(DOCUMENTS)\n",
    "    if (\n",
    "        lengths[i] <= p75_length\n",
    "        and\n",
    "        lengths[i] >= p25_length\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num ordinary documents: 1728 (50.15% of all 3446)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Num ordinary documents: {len(ORDINARY_DOCUMENTS)}'\n",
    "    f' ({100 * len(ORDINARY_DOCUMENTS) / len(DOCUMENTS):.2f}%'\n",
    "    f' of all {len(DOCUMENTS)})'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use only `TEXT_LENGTH_FOR_COHERENCE` words for coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_LENGTH_FOR_COHERENCE = 25000\n",
    "NUM_TEST_DOCUMENTS = int(TEXT_LENGTH_FOR_COHERENCE / median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TEST_DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24975.0\n"
     ]
    }
   ],
   "source": [
    "print(median_length * NUM_TEST_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lengths\n",
    "del median_length\n",
    "del p25_length\n",
    "del p75_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to choose `NUM_TEST_DOCUMENTS` randomly, seems we need to do this several times and average the results.\n",
    "In the experiments, three seeds were used (so three times selecting documents and running models training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11221963 # 11221963 # 42 # 0\n",
    "random = np.random.RandomState(seed)\n",
    "\n",
    "TEST_DOCUMENTS = random.choice(\n",
    "    ORDINARY_DOCUMENTS, size=NUM_TEST_DOCUMENTS, replace=False\n",
    ")\n",
    "TEST_DOCUMENTS = list(TEST_DOCUMENTS)\n",
    "\n",
    "del random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59620.txt',\n",
       " '16645.txt',\n",
       " '47547.txt',\n",
       " '28543.txt',\n",
       " '34582.txt',\n",
       " '13548.txt',\n",
       " '32976.txt',\n",
       " '21798.txt',\n",
       " '48114.txt',\n",
       " '36253.txt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DOCUMENTS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Time Consumption in Case of Big Datasets (or a Bit of Shamanism. Part 2)<a id=\"optimizing-time\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Currently, if dataset is initialized with `keep_in_memory=False`, getting document text takes a lot of time (which is not acceptable if we are computing coherence scores).\n",
    "So, we need to do another trick: we retrieve text for documents *once* (for documents which we selected previously) and save the texts on disk.\n",
    "And then we modify the dataset entity in such a way so it reads these saved texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how slow things are before we try to do anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 0 ns, total: 2.14 s\n",
      "Wall time: 2.01 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vw_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59620.txt</th>\n",
       "      <td>59620.txt |@word магический заговор постсоветс...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     vw_text\n",
       "id                                                          \n",
       "59620.txt  59620.txt |@word магический заговор постсоветс..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATASET.get_vw_document(TEST_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving original `get_vw_document()` function just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET._original_get_vw_document = DATASET.get_vw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_VW_TEXTS_FOLDER_PATH = os.path.join(\n",
    "    DATASET_INTERNALS_FOLDER_PATH,\n",
    "    'cached_vw_texts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CACHED_VW_TEXTS_FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_vw_text_file_path(doc_id):\n",
    "    return os.path.join(\n",
    "        CACHED_VW_TEXTS_FOLDER_PATH, f'{doc_id}.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save text on disk.\n",
    "This may take time (especially if `keep_in_memory=False`)!\n",
    "Up to $1$ hour or even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Needed to raise RAM from 8 Gb to 16 Gb (and still 4 processes)\n",
    "for doc_id in tqdm(TEST_DOCUMENTS, total=len(TEST_DOCUMENTS), file=sys.stdout):\n",
    "    vw_doc = DATASET._original_get_vw_document(doc_id)\n",
    "    vw_doc.to_csv(cached_vw_text_file_path(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_cached_get_vw_document(self, document_id):\n",
    "    return pd.read_csv(\n",
    "        cached_vw_text_file_path(document_id),\n",
    "        index_col=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we replace `get_vw_document()` with a new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET.get_vw_document = quick_cached_get_vw_document.__get__(DATASET, Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.get_vw_document of <topicnet.cooking_machine.dataset.Dataset object at 0x7f24184c5fd0>>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET._original_get_vw_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method quick_cached_get_vw_document of <topicnet.cooking_machine.dataset.Dataset object at 0x7f24184c5fd0>>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET.get_vw_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 ms, sys: 3.36 ms, total: 6.44 ms\n",
      "Wall time: 5.42 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vw_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59620.txt</th>\n",
       "      <td>59620.txt |@word магический заговор постсоветс...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     vw_text\n",
       "id                                                          \n",
       "59620.txt  59620.txt |@word магический заговор постсоветс..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATASET.get_vw_document(TEST_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment<a id=\"experiment\"></a>\n",
    "\n",
    "Finally we are getting to the main part!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores (for Topics and Models)<a id=\"scores\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Here we define a lot of scores (which mainly differ in initial parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 20\n",
    "NUM_TOP_WORDS = 20\n",
    "MAX_NUM_OUT_WORDS = 5\n",
    "\n",
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two core coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_TOPIC_SCORE = IntratextCoherenceScore(\n",
    "    name='intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_pwt__sem_none',\n",
    "    data=DATASET,\n",
    "    documents=TEST_DOCUMENTS,\n",
    "    text_type=TextType.VW_TEXT,\n",
    "    computation_method=ComputationMethod.SEGMENT_WEIGHT,\n",
    "    word_topic_relatedness=WordTopicRelatednessType.PWT,\n",
    "    specificity_estimation=SpecificityEstimationMethod.NONE,\n",
    "    max_num_out_of_topic_words=MAX_NUM_OUT_WORDS,\n",
    "    window=WINDOW,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "\n",
    "OTHER_TOPIC_SCORES = [\n",
    "    SophisticatedTopTokensCoherenceScore(\n",
    "        name='top_tokens_coherence_score__tt_vw__wtrt_pwt__sem_none',\n",
    "        data=DATASET,\n",
    "        documents=TEST_DOCUMENTS,\n",
    "        text_type=TextType.VW_TEXT,\n",
    "        word_topic_relatedness=WordTopicRelatednessType.PWT,\n",
    "        specificity_estimation=SpecificityEstimationMethod.NONE,\n",
    "        num_top_words=NUM_TOP_WORDS,\n",
    "        window=WINDOW,\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other coherence score variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_type_ids = {\n",
    "    TextType.VW_TEXT: 'vw',\n",
    "}\n",
    "computation_method_ids = {\n",
    "    ComputationMethod.SEGMENT_WEIGHT: 'seg_weight',\n",
    "    ComputationMethod.SEGMENT_LENGTH: 'seg_length',\n",
    "    ComputationMethod.SUM_OVER_WINDOW: 'sow',\n",
    "}\n",
    "word_topic_relatedness_type_ids = {\n",
    "    WordTopicRelatednessType.PWT: 'pwt',\n",
    "    WordTopicRelatednessType.PTW: 'ptw',\n",
    "}\n",
    "specificity_estimation_method_ids = {\n",
    "    SpecificityEstimationMethod.NONE: 'none',\n",
    "    SpecificityEstimationMethod.AVERAGE: 'av',\n",
    "    SpecificityEstimationMethod.MAXIMUM: 'max',\n",
    "}\n",
    "\n",
    "\n",
    "param_combinations_intratext = list(\n",
    "    itertools.product(\n",
    "        text_type_ids,\n",
    "        computation_method_ids,\n",
    "        word_topic_relatedness_type_ids,\n",
    "        specificity_estimation_method_ids,\n",
    "    )\n",
    ")\n",
    "param_combinations_intratext.remove(\n",
    "    (\n",
    "        TextType.VW_TEXT,\n",
    "        ComputationMethod.SEGMENT_WEIGHT,\n",
    "        WordTopicRelatednessType.PWT,\n",
    "        SpecificityEstimationMethod.NONE\n",
    "    )\n",
    ")\n",
    "\n",
    "param_combinations_top_tokens = list(\n",
    "    itertools.product(\n",
    "        text_type_ids,\n",
    "        word_topic_relatedness_type_ids,\n",
    "        specificity_estimation_method_ids,\n",
    "    )\n",
    ")\n",
    "param_combinations_top_tokens.remove(\n",
    "    (\n",
    "        TextType.VW_TEXT,\n",
    "        WordTopicRelatednessType.PWT,\n",
    "        SpecificityEstimationMethod.NONE\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for param_combination in param_combinations_intratext:\n",
    "    (text_type,\n",
    "     computation_method,\n",
    "     word_topic_relatedness,\n",
    "     specificity_estimation) = param_combination\n",
    "\n",
    "    name = (\n",
    "        f'intratext_coherence_score'\n",
    "        f'__tt_{text_type_ids[text_type]}'\n",
    "        f'__cm_{computation_method_ids[computation_method]}'\n",
    "        f'__wtrt_{word_topic_relatedness_type_ids[word_topic_relatedness]}'\n",
    "        f'__sem_{specificity_estimation_method_ids[specificity_estimation]}'\n",
    "    )\n",
    "\n",
    "    OTHER_TOPIC_SCORES.append(\n",
    "        IntratextCoherenceScore(\n",
    "            name=name,\n",
    "            data=DATASET,\n",
    "            documents=TEST_DOCUMENTS,\n",
    "            text_type=text_type,\n",
    "            computation_method=computation_method,\n",
    "            word_topic_relatedness=word_topic_relatedness,\n",
    "            specificity_estimation=specificity_estimation,\n",
    "            max_num_out_of_topic_words=MAX_NUM_OUT_WORDS,\n",
    "            window=WINDOW,\n",
    "            verbose=VERBOSE,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "for param_combination in param_combinations_top_tokens:\n",
    "    (text_type,\n",
    "     word_topic_relatedness,\n",
    "     specificity_estimation) = param_combination\n",
    "\n",
    "    name = (\n",
    "        f'top_tokens_coherence_score'\n",
    "        f'__tt_{text_type_ids[text_type]}'\n",
    "        f'__wtrt_{word_topic_relatedness_type_ids[word_topic_relatedness]}'\n",
    "        f'__sem_{specificity_estimation_method_ids[specificity_estimation]}'\n",
    "    )\n",
    "\n",
    "    OTHER_TOPIC_SCORES.append(\n",
    "        SophisticatedTopTokensCoherenceScore(\n",
    "            name=name,\n",
    "            data=DATASET,\n",
    "            documents=TEST_DOCUMENTS,\n",
    "            text_type=text_type,\n",
    "            word_topic_relatedness=word_topic_relatedness,\n",
    "            specificity_estimation=specificity_estimation,\n",
    "            #word_cooccurrences=COOC_VALUES2,  # TODO!\n",
    "            num_top_words=NUM_TOP_WORDS,\n",
    "            window=WINDOW,\n",
    "            verbose=VERBOSE,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OTHER_TOPIC_SCORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another implementation of top-tokens-based coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently this score is particularly slow\n",
    "# So we use just one combination of parameters\n",
    "#\n",
    "# param_combinations_other_top_tokens = list(\n",
    "#     itertools.product([True, False], ['median', 'mean'], [None, 1e-7])\n",
    "# )\n",
    "\n",
    "param_combinations_other_top_tokens = list(\n",
    "    itertools.product([True], ['median'], [1e-7])\n",
    ")\n",
    "\n",
    "if len(COOC_VALUES) > 0:  # with pre-computed coocs\n",
    "    for param_combination in param_combinations_other_top_tokens:\n",
    "        (kernel,\n",
    "         average,\n",
    "         active_topic_threshold) = param_combination\n",
    "\n",
    "        name = (\n",
    "            f'top_tokens_coherence_other_implementation_score'\n",
    "            f'__ker_{kernel}'\n",
    "            f'__av_{average}'\n",
    "            f'__att_{active_topic_threshold}'\n",
    "        )\n",
    "\n",
    "        OTHER_TOPIC_SCORES.append(\n",
    "            SimpleTopTokensCoherenceScore(\n",
    "                name=name,\n",
    "                data=DATASET,\n",
    "                cooccurrence_values=COOC_VALUES,\n",
    "                num_top_tokens=20,\n",
    "                kernel=kernel,\n",
    "                average=average,\n",
    "                active_topic_threshold=active_topic_threshold,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OTHER_TOPIC_SCORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a pair of default ARTM scores (these ones are fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER_SCORES = [\n",
    "    SparsityPhiScore(\n",
    "        name='sparsity_phi_score'\n",
    "    ),\n",
    "    SparsityThetaScore(\n",
    "        name='sparsity_theta_score'\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Creation<a id=\"bank-creation\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Here we finally run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use only one train function here\n",
    "# Other variations are also possible\n",
    "# It would be even better to make bank using several train functions\n",
    "# However, it would also take way more time \n",
    "\n",
    "TRAIN_FUNCS = default_train_func  # default train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_INTERNALS_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_RESULTS_FOLDER_PATH = os.path.join(\n",
    "    DATASET_INTERNALS_FOLDER_PATH, 'result'\n",
    ")\n",
    "\n",
    "# File with some info about the process\n",
    "SEARCH_RESULT_FILE_PATH = os.path.join(\n",
    "    SEARCH_RESULTS_FOLDER_PATH, f'search_result__{seed}.json'\n",
    ")\n",
    "\n",
    "# Bank, with topics and their score values\n",
    "BANK_FOLDER_PATH = os.path.join(\n",
    "    SEARCH_RESULTS_FOLDER_PATH, f'bank__{seed}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/result'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_RESULTS_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/result/bank__11221963'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BANK_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SEARCH_RESULTS_FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(BANK_FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11221963"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cay vary some parameters below (for example `max_num_models` and `num_fit_iterations`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = TopicBankMethod(\n",
    "    data        = DATASET,\n",
    "    min_df_rate = 0.0,  # dictionary filtering has already been done little earlier\n",
    "    max_df_rate = 1.0,  #   so we don't want these parameters to have any effect\n",
    "\n",
    "    main_topic_score   = MAIN_TOPIC_SCORE,\n",
    "    other_topic_scores = OTHER_TOPIC_SCORES,\n",
    "    other_scores       = OTHER_SCORES,\n",
    "    documents          = TEST_DOCUMENTS,\n",
    "\n",
    "    start_model_number   = 0,\n",
    "    max_num_models       = 20,\n",
    "    one_model_num_topics = 100,\n",
    "    num_fit_iterations   = 100,  # 100 should be enough;\n",
    "                                 # however, for big data better to reduce this one\n",
    "                                 # (otherwise the process will be too slow)\n",
    "\n",
    "    topic_score_threshold_percentile = 90,\n",
    "\n",
    "    save_bank         = True,\n",
    "    save_model_topics = True,\n",
    "    save_file_path    = SEARCH_RESULT_FILE_PATH,\n",
    "    bank_folder_path  = BANK_FOLDER_PATH,\n",
    "\n",
    "    train_funcs = TRAIN_FUNCS,\n",
    "    \n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "# TODO: use Holdout Perplexity as Stop score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['optimum', 'optimum_std', 'bank_scores', 'bank_topic_scores', 'model_scores', 'model_topic_scores', 'num_bank_topics', 'num_model_topics'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/result/search_result__11221963.json'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._save_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/result/bank__11221963'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._topic_bank._path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fulfilling the search (get ready for a really long process!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:11<28:35, 71.49s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:58<38:20, 100.00s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [05:25<35:14, 96.10s/it] \u001b[A\n",
      " 16%|█▌        | 4/25 [06:38<31:14, 89.28s/it]\u001b[A\n",
      " 20%|██        | 5/25 [08:02<29:13, 87.68s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [09:27<27:29, 86.82s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [10:52<25:55, 86.41s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [12:17<24:20, 85.92s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [13:43<22:53, 85.83s/it]\u001b[A\n",
      " 40%|████      | 10/25 [15:14<21:52, 87.51s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [16:42<20:28, 87.75s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [18:09<18:56, 87.41s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [19:37<17:31, 87.59s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [22:00<19:07, 104.30s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [23:55<17:54, 107.48s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [25:52<16:32, 110.27s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [27:47<14:54, 111.79s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [29:41<13:07, 112.47s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [31:33<11:14, 112.37s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [34:02<10:16, 123.25s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [36:29<08:40, 130.23s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [38:55<06:45, 135.07s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [41:22<04:37, 138.65s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [44:07<02:26, 146.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [45:02<00:00, 108.08s/it]\u001b[A\n",
      "  5%|▌         | 1/20 [54:54<17:23:11, 3294.29s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:34<37:59, 94.98s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [04:43<47:07, 122.95s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [06:17<41:57, 114.42s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [07:49<37:41, 107.69s/it]\u001b[A\n",
      " 20%|██        | 5/25 [09:23<34:32, 103.62s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [11:01<32:17, 101.95s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [12:42<30:30, 101.69s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [14:21<28:30, 100.64s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [15:57<26:30, 99.39s/it] \u001b[A\n",
      " 40%|████      | 10/25 [17:32<24:31, 98.08s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [19:09<22:46, 97.58s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [20:43<20:56, 96.64s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [22:19<19:18, 96.53s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [24:57<21:04, 114.98s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [27:35<21:19, 127.92s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [30:13<20:31, 136.83s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [32:51<19:06, 143.25s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [35:26<17:06, 146.68s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [37:58<14:49, 148.31s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [41:03<13:17, 159.42s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [44:11<11:11, 167.76s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [47:17<08:40, 173.43s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [50:25<05:55, 177.81s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [53:32<03:00, 180.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [54:40<00:00, 131.22s/it]\u001b[A\n",
      " 10%|█         | 2/20 [2:09:11<18:12:57, 3643.20s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:00<24:12, 60.53s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:27<33:11, 86.59s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:28<28:52, 78.77s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:29<25:42, 73.47s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:32<23:23, 70.19s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:40<22:05, 69.75s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [09:04<22:11, 73.99s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [10:29<21:50, 77.10s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [11:58<21:33, 80.82s/it]\u001b[A\n",
      " 40%|████      | 10/25 [13:31<21:06, 84.46s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [15:03<20:15, 86.83s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [16:33<18:58, 87.56s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [17:56<17:17, 86.44s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [20:29<19:27, 106.16s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [22:51<19:29, 116.97s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [25:14<18:44, 124.90s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [27:31<17:07, 128.48s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [29:45<15:11, 130.15s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [32:02<13:12, 132.08s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [35:00<12:10, 146.07s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [37:50<10:12, 153.24s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [40:44<07:57, 159.32s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [43:44<05:31, 165.67s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [46:44<02:49, 169.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [47:50<00:00, 114.84s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [3:14:59<17:38:08, 3734.64s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:09<27:49, 69.57s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:50<37:07, 96.86s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [05:01<32:39, 89.07s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [06:12<29:22, 83.92s/it]\u001b[A\n",
      " 20%|██        | 5/25 [07:25<26:52, 80.61s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [08:38<24:45, 78.16s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [09:51<23:00, 76.68s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [11:02<21:16, 75.10s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [12:14<19:43, 73.98s/it]\u001b[A\n",
      " 40%|████      | 10/25 [13:25<18:19, 73.31s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [14:38<17:01, 72.94s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [15:52<15:52, 73.26s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [17:00<14:22, 71.84s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [18:56<15:36, 85.15s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [20:50<15:36, 93.63s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [22:45<15:00, 100.10s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [24:40<13:55, 104.47s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [26:34<12:32, 107.43s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [28:27<10:54, 109.15s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [30:56<10:05, 121.11s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [33:23<08:34, 128.72s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [35:50<06:43, 134.47s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [38:23<04:40, 140.02s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [41:06<02:26, 146.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [42:05<00:00, 101.03s/it]\u001b[A\n",
      " 20%|██        | 4/20 [4:17:15<16:36:01, 3735.11s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:18<31:25, 78.55s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [04:20<42:02, 109.66s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [05:46<37:34, 102.48s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [07:11<34:04, 97.36s/it] \u001b[A\n",
      " 28%|██▊       | 7/25 [11:14<25:52, 86.27s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [12:31<23:38, 83.42s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [13:52<22:03, 82.71s/it]\u001b[A\n",
      " 40%|████      | 10/25 [15:17<20:50, 83.35s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [16:40<19:26, 83.35s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [18:00<17:50, 82.35s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [19:15<16:02, 80.20s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [21:36<18:03, 98.53s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [23:56<18:28, 110.85s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [26:13<17:49, 118.79s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [28:29<16:29, 123.72s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [30:39<14:39, 125.61s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [32:48<12:40, 126.80s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [35:35<11:33, 138.71s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [38:19<09:45, 146.43s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [41:07<07:38, 152.77s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [43:53<05:13, 156.78s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [46:43<02:40, 160.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [47:42<00:00, 114.49s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [5:24:36<15:56:43, 3826.91s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:01<24:43, 61.79s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:30<33:38, 87.75s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:31<29:15, 79.82s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:32<25:58, 74.21s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:33<23:24, 70.25s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:34<21:23, 67.55s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:36<19:44, 65.79s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:42<18:40, 65.91s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [11:03<18:43, 70.24s/it]\u001b[A\n",
      " 40%|████      | 10/25 [12:20<18:04, 72.32s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [13:46<17:52, 76.63s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [15:07<16:52, 77.86s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [16:29<15:49, 79.15s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [18:46<17:39, 96.34s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [20:59<17:54, 107.48s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [23:30<18:04, 120.46s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [25:59<17:12, 129.04s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [28:29<15:48, 135.45s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [30:52<13:46, 137.73s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [33:48<12:25, 149.10s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [36:45<10:29, 157.38s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [39:47<08:14, 164.77s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [42:42<05:35, 167.79s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [45:31<02:48, 168.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [46:31<00:00, 111.66s/it]\u001b[A\n",
      " 30%|███       | 6/20 [6:29:51<14:59:06, 3853.35s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:36<38:39, 96.66s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [04:41<47:12, 123.17s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [06:18<42:15, 115.24s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [07:54<38:18, 109.43s/it]\u001b[A\n",
      " 20%|██        | 5/25 [09:27<34:51, 104.57s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [11:05<32:29, 102.61s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [12:35<29:38, 98.81s/it] \u001b[A\n",
      " 32%|███▏      | 8/25 [14:03<27:05, 95.63s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [15:38<25:27, 95.49s/it]\u001b[A\n",
      " 40%|████      | 10/25 [17:14<23:54, 95.62s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [18:52<22:25, 96.10s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [20:27<20:46, 95.91s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [21:33<17:22, 86.91s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [23:29<17:32, 95.69s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [25:24<16:53, 101.36s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [27:19<15:49, 105.54s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [29:12<14:21, 107.67s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [31:06<12:48, 109.72s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [33:01<11:07, 111.21s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [35:29<10:10, 122.19s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [37:57<08:39, 129.91s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [40:24<06:45, 135.19s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [43:12<04:50, 145.04s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [46:11<02:35, 155.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [47:29<00:00, 113.96s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [7:42:40<14:28:21, 4007.83s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:42<41:04, 102.70s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [04:57<49:55, 130.25s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [06:38<44:33, 121.54s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [08:21<40:36, 116.03s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [11:34<33:30, 105.84s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [13:09<30:47, 102.62s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [14:48<28:41, 101.26s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [16:28<26:54, 100.89s/it]\u001b[A\n",
      " 40%|████      | 10/25 [18:07<25:06, 100.42s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [19:36<22:36, 96.92s/it] \u001b[A\n",
      " 48%|████▊     | 12/25 [21:08<20:41, 95.46s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [22:39<18:51, 94.31s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [25:11<20:25, 111.37s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [27:46<20:44, 124.47s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [30:22<20:05, 133.92s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [33:00<18:50, 141.34s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [35:37<17:02, 146.02s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [38:10<14:49, 148.19s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [41:18<13:20, 160.00s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [44:01<10:44, 161.01s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [46:29<07:51, 157.00s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [48:57<05:08, 154.46s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [51:25<02:32, 152.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [52:10<00:00, 125.20s/it]\u001b[A\n",
      " 40%|████      | 8/20 [8:56:58<13:48:35, 4142.98s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:39<39:52, 99.68s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [05:01<49:54, 130.19s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [06:42<44:37, 121.70s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [08:22<40:16, 115.08s/it]\u001b[A\n",
      " 20%|██        | 5/25 [10:00<36:40, 110.02s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [11:41<33:56, 107.20s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [13:19<31:19, 104.44s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [14:54<28:48, 101.68s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [16:47<27:58, 104.88s/it]\u001b[A\n",
      " 40%|████      | 10/25 [18:33<26:19, 105.29s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [20:26<25:09, 107.79s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [22:15<23:25, 108.14s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [23:56<21:11, 105.96s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [26:57<23:31, 128.29s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [29:47<23:28, 140.81s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [32:31<22:11, 147.99s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [35:14<20:18, 152.31s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [37:58<18:11, 155.98s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [40:39<15:44, 157.41s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [43:44<13:48, 165.77s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [46:53<11:30, 172.62s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [49:53<08:44, 174.98s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [52:57<05:54, 177.48s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [56:02<02:59, 179.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [57:13<00:00, 137.32s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [10:14:37<13:07:56, 4297.90s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:02<25:05, 62.73s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:32<34:03, 88.84s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:35<29:44, 81.13s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:38<26:27, 75.57s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:42<24:04, 72.22s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:45<21:56, 69.30s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [09:01<21:27, 71.51s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [10:38<22:24, 79.07s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [12:15<22:32, 84.51s/it]\u001b[A\n",
      " 40%|████      | 10/25 [14:01<22:44, 90.94s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [15:49<22:23, 95.96s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [17:30<21:07, 97.50s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [19:09<19:36, 98.04s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [22:07<22:21, 121.99s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [25:07<23:12, 139.22s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [28:00<22:26, 149.63s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [30:41<20:23, 152.99s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [33:24<18:10, 155.82s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [36:26<16:22, 163.75s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [43:02<12:04, 181.13s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [46:17<09:16, 185.41s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [49:30<06:15, 187.73s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [52:43<03:09, 189.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [54:04<00:00, 129.79s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [11:28:20<12:02:34, 4335.42s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:33<37:28, 93.70s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [04:15<43:43, 114.05s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [05:16<36:00, 98.22s/it] \u001b[A\n",
      " 16%|█▌        | 4/25 [06:18<30:33, 87.32s/it]\u001b[A\n",
      " 20%|██        | 5/25 [07:20<26:32, 79.64s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [08:22<23:32, 74.34s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [09:24<21:11, 70.65s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [10:25<19:13, 67.83s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [11:26<17:34, 65.89s/it]\u001b[A\n",
      " 40%|████      | 10/25 [12:29<16:15, 65.01s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [13:31<14:56, 64.01s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [14:33<13:46, 63.54s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [15:35<12:35, 62.95s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [17:31<14:26, 78.81s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [19:25<14:54, 89.47s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [21:23<14:41, 97.93s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [23:27<14:05, 105.72s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [26:07<14:13, 121.99s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [28:55<13:36, 136.03s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [32:14<12:54, 154.83s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [35:28<11:06, 166.66s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [38:44<08:46, 175.38s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [41:58<06:01, 180.97s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [45:10<03:04, 184.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [46:33<00:00, 111.75s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [12:40:17<10:49:28, 4329.84s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:22<33:03, 82.64s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [04:26<43:21, 113.10s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [05:43<37:25, 102.09s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [07:00<33:05, 94.54s/it] \u001b[A\n",
      " 20%|██        | 5/25 [08:19<30:02, 90.13s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [09:38<27:24, 86.56s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [10:59<25:31, 85.06s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [12:21<23:48, 84.02s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [13:43<22:17, 83.59s/it]\u001b[A\n",
      " 40%|████      | 10/25 [15:04<20:39, 82.60s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [16:28<19:24, 83.18s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [17:50<17:56, 82.80s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [19:16<16:43, 83.65s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [21:51<19:16, 105.17s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [24:27<20:02, 120.25s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [26:22<17:49, 118.85s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [28:19<15:46, 118.25s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [30:18<13:49, 118.44s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [32:14<11:46, 117.68s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [34:45<10:38, 127.74s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [37:14<08:56, 134.07s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [39:41<06:54, 138.04s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [42:09<04:41, 140.88s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [44:39<02:23, 143.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [45:26<00:00, 109.05s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [13:52:02<9:36:18, 4322.34s/it] \n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:55<46:21, 115.88s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [05:19<54:29, 142.15s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [07:11<48:51, 133.24s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [08:57<43:43, 124.91s/it]\u001b[A\n",
      " 20%|██        | 5/25 [10:49<40:22, 121.12s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [12:37<37:05, 117.14s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [14:24<34:14, 114.14s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [16:06<31:17, 110.44s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [17:46<28:39, 107.49s/it]\u001b[A\n",
      " 40%|████      | 10/25 [19:26<26:18, 105.20s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [21:08<24:18, 104.19s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [22:50<22:24, 103.42s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [24:30<20:30, 102.52s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [27:16<22:17, 121.58s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [29:56<22:11, 133.18s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [32:32<20:59, 139.92s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [35:04<19:07, 143.43s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [37:47<17:26, 149.50s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [40:30<15:20, 153.42s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [43:38<13:39, 163.85s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [46:49<11:27, 171.99s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [49:58<08:51, 177.11s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [52:53<05:52, 176.28s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [55:21<02:48, 168.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [56:08<00:00, 134.72s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [15:10:42<8:38:10, 4441.53s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:01<24:28, 61.20s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:30<33:38, 87.74s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:35<29:37, 80.82s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:37<26:18, 75.19s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:39<23:42, 71.12s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:40<21:35, 68.17s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:42<19:54, 66.36s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:44<18:25, 65.03s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:46<17:07, 64.24s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:49<15:55, 63.70s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:50<14:41, 62.95s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:56<13:49, 63.82s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [14:58<12:40, 63.41s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:54<14:29, 79.06s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:51<15:04, 90.42s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:51<14:53, 99.25s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:47<13:54, 104.33s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:45<12:38, 108.34s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:45<11:12, 112.01s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [29:14<10:14, 123.00s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [31:42<08:42, 130.55s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [34:09<06:46, 135.59s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [36:37<04:38, 139.19s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [39:05<02:21, 141.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:50<00:00, 95.63s/it] \u001b[A\n",
      " 70%|███████   | 14/20 [16:06:33<6:51:27, 4114.53s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:01<24:43, 61.83s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:29<33:37, 87.72s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:31<29:18, 79.93s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:34<26:10, 74.80s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:37<23:42, 71.11s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:39<21:41, 68.51s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:41<19:58, 66.58s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:43<18:25, 65.04s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:45<17:06, 64.17s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:47<15:54, 63.67s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:49<14:45, 63.24s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:51<13:36, 62.77s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [14:53<12:29, 62.48s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:48<14:19, 78.14s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:42<14:50, 89.02s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:40<14:38, 97.61s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:36<13:45, 103.19s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:32<12:29, 107.05s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:29<11:00, 110.04s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [28:59<10:09, 121.98s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [31:29<08:41, 130.47s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [33:57<06:47, 135.71s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [36:27<04:39, 139.91s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [38:56<02:22, 142.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:45<00:00, 95.42s/it] \u001b[A\n",
      " 75%|███████▌  | 15/20 [17:02:16<5:23:34, 3882.95s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:01<24:47, 61.97s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:31<33:52, 88.35s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:33<29:25, 80.27s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:35<26:10, 74.78s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:37<23:40, 71.01s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:39<21:39, 68.42s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:41<19:56, 66.47s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:43<18:24, 64.97s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:44<17:03, 63.97s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:47<15:53, 63.54s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:49<14:43, 63.10s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:51<13:34, 62.65s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [14:52<12:27, 62.27s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:48<14:22, 78.38s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:42<14:49, 88.97s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:38<14:33, 97.08s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:34<13:42, 102.86s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:30<12:27, 106.79s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:25<10:55, 109.29s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [28:57<10:11, 122.22s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [31:28<08:42, 130.70s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [33:56<06:47, 135.85s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [36:24<04:39, 139.51s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [38:52<02:22, 142.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:37<00:00, 95.12s/it] \u001b[A\n",
      " 80%|████████  | 16/20 [17:58:30<4:08:41, 3730.39s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:01<24:41, 61.72s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:30<33:40, 87.84s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:32<29:20, 80.02s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:33<26:02, 74.39s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:34<23:29, 70.48s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:36<21:27, 67.78s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:38<19:47, 65.96s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:39<18:19, 64.68s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:42<17:03, 63.95s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:46<15:59, 63.96s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:47<14:45, 63.23s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:49<13:38, 62.99s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [14:52<12:33, 62.76s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:46<14:21, 78.34s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:42<14:55, 89.55s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:36<14:31, 96.89s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:31<13:37, 102.24s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:26<12:23, 106.16s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:22<10:54, 109.02s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [28:52<10:07, 121.51s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [31:21<08:37, 129.49s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [33:49<06:45, 135.21s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [36:19<04:38, 139.45s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [38:48<02:22, 142.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:35<00:00, 95.04s/it] \u001b[A\n",
      " 85%|████████▌ | 17/20 [18:54:08<3:00:37, 3612.44s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:03<25:30, 63.77s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:34<34:25, 89.81s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:37<29:57, 81.69s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:38<26:28, 75.64s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:40<23:53, 71.66s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:43<21:51, 69.04s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:46<20:08, 67.13s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:48<18:36, 65.66s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:50<17:13, 64.59s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:53<15:59, 63.96s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:54<14:45, 63.24s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:57<13:38, 62.96s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [15:00<12:35, 62.92s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:57<14:31, 79.20s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:53<15:01, 90.17s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:49<14:42, 98.07s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:45<13:48, 103.57s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:41<12:31, 107.32s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:36<10:56, 109.48s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [29:06<10:07, 121.52s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [31:34<08:37, 129.42s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [34:00<06:44, 134.68s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [36:27<04:36, 138.16s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [38:55<02:21, 141.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:39<00:00, 95.20s/it] \u001b[A\n",
      " 90%|█████████ | 18/20 [19:49:29<1:57:30, 3525.26s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:00<24:16, 60.67s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:27<33:07, 86.41s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:28<28:54, 78.85s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:29<25:41, 73.43s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:30<23:13, 69.68s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:31<21:15, 67.15s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:32<19:34, 65.27s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:33<18:09, 64.07s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:37<17:07, 64.21s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:39<15:49, 63.31s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:40<14:38, 62.76s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:42<13:31, 62.40s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [14:43<12:23, 61.97s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:36<14:11, 77.40s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:28<14:38, 87.81s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:22<14:19, 95.45s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:16<13:29, 101.21s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:09<12:13, 104.76s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:02<10:43, 107.23s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [28:29<09:55, 119.17s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [30:56<08:30, 127.55s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [33:23<06:39, 133.32s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [35:50<04:34, 137.42s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [38:17<02:20, 140.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:00<00:00, 93.64s/it] \u001b[A\n",
      " 95%|█████████▌| 19/20 [20:44:10<57:31, 3451.85s/it]  \n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [01:08<27:27, 68.63s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [03:36<35:25, 92.40s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [04:37<30:23, 82.89s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [05:38<26:41, 76.27s/it]\u001b[A\n",
      " 20%|██        | 5/25 [06:38<23:51, 71.58s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [07:39<21:39, 68.38s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [08:41<19:56, 66.50s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [09:43<18:25, 65.02s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [10:43<16:57, 63.60s/it]\u001b[A\n",
      " 40%|████      | 10/25 [11:47<15:54, 63.63s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [12:49<14:47, 63.36s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [13:51<13:37, 62.86s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [14:52<12:28, 62.39s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [16:49<14:24, 78.56s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [18:45<15:00, 90.00s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [20:41<14:38, 97.66s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [22:33<13:36, 102.02s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [24:28<12:20, 105.82s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [26:25<10:55, 109.17s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [28:54<10:05, 121.19s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [31:22<08:36, 129.19s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [33:50<06:44, 134.86s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [36:19<04:38, 139.20s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [38:49<02:22, 142.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../topnum/scores/simple_toptok_coherence_score.py:120: UserWarning: The parameter `documents` is not used by SimpleTopTokensCoherenceScore (it is kept for compatibility purposes)\n",
      "  'The parameter `documents` is not used by SimpleTopTokensCoherenceScore'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 25/25 [39:32<00:00, 94.90s/it] \u001b[A\n",
      "100%|██████████| 20/20 [21:39:25<00:00, 3898.28s/it]\n",
      "CPU times: user 1d 10h 46s, sys: 3h 46min 9s, total: 1d 13h 46min 55s\n",
      "Wall time: 21h 39min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer.search_for_optimum(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What topics we have in bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">@word</th>\n",
       "      <th>перикл</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>леагр</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ольвийский</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>амфора</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>вазопись</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  \\\n",
       "@word перикл          0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "      леагр           0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "      ольвийский      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "      амфора          0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "      вазопись        0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "                  topic_6  topic_7  topic_8  topic_9  \n",
       "@word перикл          0.0      0.0      0.0      0.0  \n",
       "      леагр           0.0      0.0      0.0      0.0  \n",
       "      ольвийский      0.0      0.0      0.0      0.0  \n",
       "      амфора          0.0      0.0      0.0      0.0  \n",
       "      вазопись        0.0      0.0      0.0      0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._topic_bank.view_topics().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_topics = optimizer._topic_bank.view_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 15)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">@default_class</th>\n",
       "      <th>както</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.497794e-11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.985204e-08</td>\n",
       "      <td>2.223492e-14</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.414336e-06</td>\n",
       "      <td>4.583967e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>гравитационный</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022702</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>жена</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.217113e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.405896e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091175e-13</td>\n",
       "      <td>8.552548e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>продолжительность</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.470690e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.282006e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.474610e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>одновременно</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>5.159791e-06</td>\n",
       "      <td>7.805681e-09</td>\n",
       "      <td>3.031660e-06</td>\n",
       "      <td>5.124066e-09</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>8.667931e-13</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>1.033011e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.076379e-05</td>\n",
       "      <td>1.407077e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  topic_0  topic_1       topic_2  \\\n",
       "@default_class както                  0.0  0.00000  5.497794e-11   \n",
       "               гравитационный         0.0  0.00000  0.000000e+00   \n",
       "               жена                   0.0  0.00139  0.000000e+00   \n",
       "               продолжительность      0.0  0.00000  0.000000e+00   \n",
       "               одновременно           0.0  0.00075  5.159791e-06   \n",
       "\n",
       "                                       topic_3       topic_4       topic_5  \\\n",
       "@default_class както              0.000000e+00  1.985204e-08  2.223492e-14   \n",
       "               гравитационный     0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "               жена               0.000000e+00  0.000000e+00  1.217113e-04   \n",
       "               продолжительность  3.470690e-15  0.000000e+00  0.000000e+00   \n",
       "               одновременно       7.805681e-09  3.031660e-06  5.124066e-09   \n",
       "\n",
       "                                   topic_6       topic_7   topic_8   topic_9  \\\n",
       "@default_class както              0.000009  0.000000e+00  0.000000  0.000000   \n",
       "               гравитационный     0.000000  0.000000e+00  0.000000  0.022702   \n",
       "               жена               0.000000  3.405896e-15  0.000000  0.000000   \n",
       "               продолжительность  0.000000  4.282006e-04  0.000000  0.000000   \n",
       "               одновременно       0.000146  8.667931e-13  0.000935  0.000564   \n",
       "\n",
       "                                      topic_10  topic_11  topic_12  \\\n",
       "@default_class както              0.000000e+00       0.0       0.0   \n",
       "               гравитационный     0.000000e+00       0.0       0.0   \n",
       "               жена               0.000000e+00       0.0       0.0   \n",
       "               продолжительность  3.474610e-08       0.0       0.0   \n",
       "               одновременно       1.033011e-07       0.0       0.0   \n",
       "\n",
       "                                      topic_13      topic_14  \n",
       "@default_class както              2.414336e-06  4.583967e-11  \n",
       "               гравитационный     0.000000e+00  0.000000e+00  \n",
       "               жена               1.091175e-13  8.552548e-05  \n",
       "               продолжительность  0.000000e+00  0.000000e+00  \n",
       "               одновременно       2.076379e-05  1.407077e-09  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@default_class  век             0.229964\n",
       "                xx              0.053898\n",
       "                xix             0.051478\n",
       "                начало          0.043227\n",
       "                первый          0.038318\n",
       "                конец           0.035413\n",
       "                второй          0.019743\n",
       "                середина        0.018641\n",
       "                время           0.017661\n",
       "                половина        0.017191\n",
       "                xviii           0.013792\n",
       "                классический    0.013726\n",
       "                xvii            0.012661\n",
       "                возникать       0.011614\n",
       "                эпоха           0.010723\n",
       "                хх              0.010398\n",
       "                новый           0.010231\n",
       "                столетие        0.008743\n",
       "                образ           0.008709\n",
       "                период          0.008326\n",
       "Name: topic_14, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_topics['topic_14'].sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And topic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kernel_size</th>\n",
       "      <td>229.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_pwt__sem_none</th>\n",
       "      <td>0.129687</td>\n",
       "      <td>0.123335</td>\n",
       "      <td>0.468014</td>\n",
       "      <td>0.249535</td>\n",
       "      <td>0.211076</td>\n",
       "      <td>0.246103</td>\n",
       "      <td>0.084003</td>\n",
       "      <td>0.103003</td>\n",
       "      <td>0.149715</td>\n",
       "      <td>0.085349</td>\n",
       "      <td>0.102518</td>\n",
       "      <td>0.177264</td>\n",
       "      <td>0.116741</td>\n",
       "      <td>0.102491</td>\n",
       "      <td>0.099711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_score__tt_vw__wtrt_pwt__sem_none</th>\n",
       "      <td>1.133654</td>\n",
       "      <td>0.667880</td>\n",
       "      <td>1.629793</td>\n",
       "      <td>0.835258</td>\n",
       "      <td>0.835292</td>\n",
       "      <td>0.806799</td>\n",
       "      <td>0.434416</td>\n",
       "      <td>0.921406</td>\n",
       "      <td>0.700194</td>\n",
       "      <td>0.409935</td>\n",
       "      <td>0.473712</td>\n",
       "      <td>0.614014</td>\n",
       "      <td>0.583990</td>\n",
       "      <td>0.500487</td>\n",
       "      <td>0.975982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_pwt__sem_av</th>\n",
       "      <td>0.127684</td>\n",
       "      <td>0.121866</td>\n",
       "      <td>0.462997</td>\n",
       "      <td>0.246569</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.243621</td>\n",
       "      <td>0.082943</td>\n",
       "      <td>0.101184</td>\n",
       "      <td>0.147921</td>\n",
       "      <td>0.084183</td>\n",
       "      <td>0.101309</td>\n",
       "      <td>0.173868</td>\n",
       "      <td>0.113667</td>\n",
       "      <td>0.101379</td>\n",
       "      <td>0.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_pwt__sem_max</th>\n",
       "      <td>0.120821</td>\n",
       "      <td>0.114016</td>\n",
       "      <td>0.462851</td>\n",
       "      <td>0.245817</td>\n",
       "      <td>0.116343</td>\n",
       "      <td>0.245570</td>\n",
       "      <td>0.079723</td>\n",
       "      <td>0.083172</td>\n",
       "      <td>0.143387</td>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.096531</td>\n",
       "      <td>0.103924</td>\n",
       "      <td>0.100431</td>\n",
       "      <td>0.100097</td>\n",
       "      <td>0.080213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_ptw__sem_none</th>\n",
       "      <td>0.463680</td>\n",
       "      <td>0.783485</td>\n",
       "      <td>0.763705</td>\n",
       "      <td>0.481053</td>\n",
       "      <td>0.594415</td>\n",
       "      <td>0.914586</td>\n",
       "      <td>0.645213</td>\n",
       "      <td>0.466021</td>\n",
       "      <td>0.734765</td>\n",
       "      <td>0.765880</td>\n",
       "      <td>0.903625</td>\n",
       "      <td>0.845633</td>\n",
       "      <td>0.492089</td>\n",
       "      <td>0.642143</td>\n",
       "      <td>0.650835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_ptw__sem_av</th>\n",
       "      <td>0.452750</td>\n",
       "      <td>0.771956</td>\n",
       "      <td>0.753009</td>\n",
       "      <td>0.470381</td>\n",
       "      <td>0.583590</td>\n",
       "      <td>0.903922</td>\n",
       "      <td>0.633082</td>\n",
       "      <td>0.455416</td>\n",
       "      <td>0.723801</td>\n",
       "      <td>0.753463</td>\n",
       "      <td>0.890064</td>\n",
       "      <td>0.830870</td>\n",
       "      <td>0.479616</td>\n",
       "      <td>0.631021</td>\n",
       "      <td>0.636409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_ptw__sem_max</th>\n",
       "      <td>0.385764</td>\n",
       "      <td>0.677018</td>\n",
       "      <td>0.718970</td>\n",
       "      <td>0.418170</td>\n",
       "      <td>0.345956</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.549258</td>\n",
       "      <td>0.345888</td>\n",
       "      <td>0.661878</td>\n",
       "      <td>0.648797</td>\n",
       "      <td>0.772138</td>\n",
       "      <td>0.636360</td>\n",
       "      <td>0.417972</td>\n",
       "      <td>0.574683</td>\n",
       "      <td>0.525002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_length__wtrt_pwt__sem_none</th>\n",
       "      <td>1.092971</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>1.069599</td>\n",
       "      <td>1.067143</td>\n",
       "      <td>1.082481</td>\n",
       "      <td>1.066313</td>\n",
       "      <td>1.213043</td>\n",
       "      <td>1.060516</td>\n",
       "      <td>1.096398</td>\n",
       "      <td>1.241758</td>\n",
       "      <td>1.356061</td>\n",
       "      <td>1.476266</td>\n",
       "      <td>1.247272</td>\n",
       "      <td>1.112245</td>\n",
       "      <td>1.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_length__wtrt_pwt__sem_av</th>\n",
       "      <td>1.092971</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>1.069599</td>\n",
       "      <td>1.067143</td>\n",
       "      <td>1.082481</td>\n",
       "      <td>1.066313</td>\n",
       "      <td>1.213043</td>\n",
       "      <td>1.060516</td>\n",
       "      <td>1.096398</td>\n",
       "      <td>1.241758</td>\n",
       "      <td>1.356061</td>\n",
       "      <td>1.476266</td>\n",
       "      <td>1.247272</td>\n",
       "      <td>1.112245</td>\n",
       "      <td>1.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_length__wtrt_pwt__sem_max</th>\n",
       "      <td>1.092971</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>1.069599</td>\n",
       "      <td>1.067143</td>\n",
       "      <td>1.082481</td>\n",
       "      <td>1.066313</td>\n",
       "      <td>1.213043</td>\n",
       "      <td>1.060516</td>\n",
       "      <td>1.096398</td>\n",
       "      <td>1.241758</td>\n",
       "      <td>1.356061</td>\n",
       "      <td>1.476266</td>\n",
       "      <td>1.247272</td>\n",
       "      <td>1.112245</td>\n",
       "      <td>1.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_length__wtrt_ptw__sem_none</th>\n",
       "      <td>1.092971</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>1.069599</td>\n",
       "      <td>1.067143</td>\n",
       "      <td>1.082481</td>\n",
       "      <td>1.066313</td>\n",
       "      <td>1.213043</td>\n",
       "      <td>1.060516</td>\n",
       "      <td>1.096398</td>\n",
       "      <td>1.241758</td>\n",
       "      <td>1.356061</td>\n",
       "      <td>1.476266</td>\n",
       "      <td>1.247272</td>\n",
       "      <td>1.112245</td>\n",
       "      <td>1.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_length__wtrt_ptw__sem_av</th>\n",
       "      <td>1.092971</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>1.069599</td>\n",
       "      <td>1.067143</td>\n",
       "      <td>1.082481</td>\n",
       "      <td>1.066313</td>\n",
       "      <td>1.213043</td>\n",
       "      <td>1.060516</td>\n",
       "      <td>1.096398</td>\n",
       "      <td>1.241758</td>\n",
       "      <td>1.356061</td>\n",
       "      <td>1.476266</td>\n",
       "      <td>1.247272</td>\n",
       "      <td>1.112245</td>\n",
       "      <td>1.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_seg_length__wtrt_ptw__sem_max</th>\n",
       "      <td>1.092971</td>\n",
       "      <td>1.152834</td>\n",
       "      <td>1.069599</td>\n",
       "      <td>1.067143</td>\n",
       "      <td>1.082481</td>\n",
       "      <td>1.066313</td>\n",
       "      <td>1.213043</td>\n",
       "      <td>1.060516</td>\n",
       "      <td>1.096398</td>\n",
       "      <td>1.241758</td>\n",
       "      <td>1.356061</td>\n",
       "      <td>1.476266</td>\n",
       "      <td>1.247272</td>\n",
       "      <td>1.112245</td>\n",
       "      <td>1.442545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_sow__wtrt_pwt__sem_none</th>\n",
       "      <td>0.149405</td>\n",
       "      <td>0.135239</td>\n",
       "      <td>0.506719</td>\n",
       "      <td>0.277130</td>\n",
       "      <td>0.230360</td>\n",
       "      <td>0.278230</td>\n",
       "      <td>0.113456</td>\n",
       "      <td>0.119589</td>\n",
       "      <td>0.165493</td>\n",
       "      <td>0.098369</td>\n",
       "      <td>0.113682</td>\n",
       "      <td>0.187889</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.121448</td>\n",
       "      <td>0.123747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_sow__wtrt_pwt__sem_av</th>\n",
       "      <td>0.129879</td>\n",
       "      <td>0.115653</td>\n",
       "      <td>0.483348</td>\n",
       "      <td>0.258036</td>\n",
       "      <td>0.208231</td>\n",
       "      <td>0.257433</td>\n",
       "      <td>0.095994</td>\n",
       "      <td>0.100934</td>\n",
       "      <td>0.147807</td>\n",
       "      <td>0.080106</td>\n",
       "      <td>0.094437</td>\n",
       "      <td>0.167958</td>\n",
       "      <td>0.102463</td>\n",
       "      <td>0.100667</td>\n",
       "      <td>0.104875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_sow__wtrt_pwt__sem_max</th>\n",
       "      <td>-0.571706</td>\n",
       "      <td>-0.647320</td>\n",
       "      <td>-0.181738</td>\n",
       "      <td>-0.339688</td>\n",
       "      <td>-0.670294</td>\n",
       "      <td>-0.440744</td>\n",
       "      <td>-0.461628</td>\n",
       "      <td>-0.599578</td>\n",
       "      <td>-0.525545</td>\n",
       "      <td>-0.513798</td>\n",
       "      <td>-0.574491</td>\n",
       "      <td>-0.615529</td>\n",
       "      <td>-0.620008</td>\n",
       "      <td>-0.690838</td>\n",
       "      <td>-0.588022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_sow__wtrt_ptw__sem_none</th>\n",
       "      <td>0.614931</td>\n",
       "      <td>0.908630</td>\n",
       "      <td>0.905199</td>\n",
       "      <td>0.617715</td>\n",
       "      <td>0.738819</td>\n",
       "      <td>1.190128</td>\n",
       "      <td>0.909712</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.853647</td>\n",
       "      <td>0.925611</td>\n",
       "      <td>1.090329</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>0.539923</td>\n",
       "      <td>0.890184</td>\n",
       "      <td>0.861727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_sow__wtrt_ptw__sem_av</th>\n",
       "      <td>0.418786</td>\n",
       "      <td>0.721543</td>\n",
       "      <td>0.708845</td>\n",
       "      <td>0.418287</td>\n",
       "      <td>0.539387</td>\n",
       "      <td>0.993577</td>\n",
       "      <td>0.718625</td>\n",
       "      <td>0.421874</td>\n",
       "      <td>0.675967</td>\n",
       "      <td>0.727809</td>\n",
       "      <td>0.897677</td>\n",
       "      <td>0.729241</td>\n",
       "      <td>0.352384</td>\n",
       "      <td>0.690184</td>\n",
       "      <td>0.664889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intratext_coherence_score__tt_vw__cm_sow__wtrt_ptw__sem_max</th>\n",
       "      <td>-6.451642</td>\n",
       "      <td>-6.104594</td>\n",
       "      <td>-5.935633</td>\n",
       "      <td>-6.477285</td>\n",
       "      <td>-7.503458</td>\n",
       "      <td>-5.649234</td>\n",
       "      <td>-5.685040</td>\n",
       "      <td>-6.472778</td>\n",
       "      <td>-5.917510</td>\n",
       "      <td>-5.621622</td>\n",
       "      <td>-5.912393</td>\n",
       "      <td>-6.323709</td>\n",
       "      <td>-6.598900</td>\n",
       "      <td>-6.158614</td>\n",
       "      <td>-6.135952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_score__tt_vw__wtrt_pwt__sem_av</th>\n",
       "      <td>1.085201</td>\n",
       "      <td>0.207429</td>\n",
       "      <td>0.924944</td>\n",
       "      <td>0.711773</td>\n",
       "      <td>0.631549</td>\n",
       "      <td>0.804109</td>\n",
       "      <td>0.434745</td>\n",
       "      <td>0.751648</td>\n",
       "      <td>0.496965</td>\n",
       "      <td>0.313646</td>\n",
       "      <td>0.339534</td>\n",
       "      <td>0.532214</td>\n",
       "      <td>0.583990</td>\n",
       "      <td>0.432630</td>\n",
       "      <td>0.894360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_score__tt_vw__wtrt_pwt__sem_max</th>\n",
       "      <td>0.216871</td>\n",
       "      <td>0.087710</td>\n",
       "      <td>0.133331</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>0.204369</td>\n",
       "      <td>0.586044</td>\n",
       "      <td>0.182890</td>\n",
       "      <td>0.139113</td>\n",
       "      <td>0.333415</td>\n",
       "      <td>0.225083</td>\n",
       "      <td>0.292303</td>\n",
       "      <td>0.235531</td>\n",
       "      <td>0.224648</td>\n",
       "      <td>0.221136</td>\n",
       "      <td>0.689570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_score__tt_vw__wtrt_ptw__sem_none</th>\n",
       "      <td>0.312881</td>\n",
       "      <td>0.089159</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.133407</td>\n",
       "      <td>0.298121</td>\n",
       "      <td>0.432108</td>\n",
       "      <td>0.406538</td>\n",
       "      <td>0.127957</td>\n",
       "      <td>0.428056</td>\n",
       "      <td>0.293603</td>\n",
       "      <td>0.438889</td>\n",
       "      <td>0.258486</td>\n",
       "      <td>0.188524</td>\n",
       "      <td>0.100785</td>\n",
       "      <td>0.602910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_score__tt_vw__wtrt_ptw__sem_av</th>\n",
       "      <td>0.312881</td>\n",
       "      <td>0.089159</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.133407</td>\n",
       "      <td>0.298121</td>\n",
       "      <td>0.432108</td>\n",
       "      <td>0.406538</td>\n",
       "      <td>0.127957</td>\n",
       "      <td>0.428056</td>\n",
       "      <td>0.293603</td>\n",
       "      <td>0.438889</td>\n",
       "      <td>0.258486</td>\n",
       "      <td>0.188524</td>\n",
       "      <td>0.100785</td>\n",
       "      <td>0.602910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_score__tt_vw__wtrt_ptw__sem_max</th>\n",
       "      <td>0.338780</td>\n",
       "      <td>0.129497</td>\n",
       "      <td>0.558454</td>\n",
       "      <td>0.151350</td>\n",
       "      <td>0.403012</td>\n",
       "      <td>0.482340</td>\n",
       "      <td>0.168771</td>\n",
       "      <td>0.165393</td>\n",
       "      <td>0.367842</td>\n",
       "      <td>0.133010</td>\n",
       "      <td>0.208253</td>\n",
       "      <td>0.313572</td>\n",
       "      <td>0.287957</td>\n",
       "      <td>0.206843</td>\n",
       "      <td>0.655232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_tokens_coherence_other_implementation_score__ker_True__av_median__att_1e-07</th>\n",
       "      <td>0.184876</td>\n",
       "      <td>0.060949</td>\n",
       "      <td>0.243379</td>\n",
       "      <td>0.170201</td>\n",
       "      <td>0.063091</td>\n",
       "      <td>0.109392</td>\n",
       "      <td>0.074855</td>\n",
       "      <td>0.061806</td>\n",
       "      <td>0.217911</td>\n",
       "      <td>0.045327</td>\n",
       "      <td>0.155120</td>\n",
       "      <td>0.213721</td>\n",
       "      <td>0.231643</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.143040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distance_to_nearest</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.957545</td>\n",
       "      <td>0.969069</td>\n",
       "      <td>0.957263</td>\n",
       "      <td>0.958783</td>\n",
       "      <td>0.936528</td>\n",
       "      <td>0.931596</td>\n",
       "      <td>0.939177</td>\n",
       "      <td>0.945021</td>\n",
       "      <td>0.924911</td>\n",
       "      <td>0.917157</td>\n",
       "      <td>0.506945</td>\n",
       "      <td>0.958058</td>\n",
       "      <td>0.898429</td>\n",
       "      <td>0.533940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       topic_0     topic_1  \\\n",
       "kernel_size                                         229.000000  408.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.129687    0.123335   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    1.133654    0.667880   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.127684    0.121866   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.120821    0.114016   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.463680    0.783485   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.452750    0.771956   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.385764    0.677018   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.092971    1.152834   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.092971    1.152834   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.092971    1.152834   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.092971    1.152834   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.092971    1.152834   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.092971    1.152834   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.149405    0.135239   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.129879    0.115653   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.571706   -0.647320   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.614931    0.908630   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.418786    0.721543   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -6.451642   -6.104594   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    1.085201    0.207429   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.216871    0.087710   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.312881    0.089159   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.312881    0.089159   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.338780    0.129497   \n",
       "top_tokens_coherence_other_implementation_score...    0.184876    0.060949   \n",
       "distance_to_nearest                                   0.000000    0.957545   \n",
       "\n",
       "                                                       topic_2     topic_3  \\\n",
       "kernel_size                                         102.000000  157.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.468014    0.249535   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    1.629793    0.835258   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.462997    0.246569   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.462851    0.245817   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.763705    0.481053   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.753009    0.470381   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.718970    0.418170   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.069599    1.067143   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.069599    1.067143   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.069599    1.067143   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.069599    1.067143   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.069599    1.067143   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.069599    1.067143   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.506719    0.277130   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.483348    0.258036   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.181738   -0.339688   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.905199    0.617715   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.708845    0.418287   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -5.935633   -6.477285   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.924944    0.711773   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.133331    0.112849   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.278846    0.133407   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.278846    0.133407   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.558454    0.151350   \n",
       "top_tokens_coherence_other_implementation_score...    0.243379    0.170201   \n",
       "distance_to_nearest                                   0.969069    0.957263   \n",
       "\n",
       "                                                       topic_4     topic_5  \\\n",
       "kernel_size                                         235.000000  302.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.211076    0.246103   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.835292    0.806799   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.207621    0.243621   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.116343    0.245570   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.594415    0.914586   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.583590    0.903922   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.345956    0.879365   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.082481    1.066313   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.082481    1.066313   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.082481    1.066313   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.082481    1.066313   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.082481    1.066313   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.082481    1.066313   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.230360    0.278230   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.208231    0.257433   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.670294   -0.440744   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.738819    1.190128   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.539387    0.993577   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -7.503458   -5.649234   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.631549    0.804109   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.204369    0.586044   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.298121    0.432108   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.298121    0.432108   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.403012    0.482340   \n",
       "top_tokens_coherence_other_implementation_score...    0.063091    0.109392   \n",
       "distance_to_nearest                                   0.958783    0.936528   \n",
       "\n",
       "                                                       topic_6     topic_7  \\\n",
       "kernel_size                                         345.000000  347.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.084003    0.103003   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.434416    0.921406   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.082943    0.101184   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.079723    0.083172   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.645213    0.466021   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.633082    0.455416   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.549258    0.345888   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.213043    1.060516   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.213043    1.060516   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.213043    1.060516   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.213043    1.060516   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.213043    1.060516   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.213043    1.060516   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.113456    0.119589   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.095994    0.100934   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.461628   -0.599578   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.909712    0.618700   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.718625    0.421874   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -5.685040   -6.472778   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.434745    0.751648   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.182890    0.139113   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.406538    0.127957   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.406538    0.127957   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.168771    0.165393   \n",
       "top_tokens_coherence_other_implementation_score...    0.074855    0.061806   \n",
       "distance_to_nearest                                   0.931596    0.939177   \n",
       "\n",
       "                                                       topic_8     topic_9  \\\n",
       "kernel_size                                         283.000000  391.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.149715    0.085349   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.700194    0.409935   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.147921    0.084183   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.143387    0.075810   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.734765    0.765880   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.723801    0.753463   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.661878    0.648797   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.096398    1.241758   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.096398    1.241758   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.096398    1.241758   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.096398    1.241758   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.096398    1.241758   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.096398    1.241758   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.165493    0.098369   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.147807    0.080106   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.525545   -0.513798   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.853647    0.925611   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.675967    0.727809   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -5.917510   -5.621622   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.496965    0.313646   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.333415    0.225083   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.428056    0.293603   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.428056    0.293603   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.367842    0.133010   \n",
       "top_tokens_coherence_other_implementation_score...    0.217911    0.045327   \n",
       "distance_to_nearest                                   0.945021    0.924911   \n",
       "\n",
       "                                                      topic_10   topic_11  \\\n",
       "kernel_size                                         338.000000  59.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.102518   0.177264   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.473712   0.614014   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.101309   0.173868   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.096531   0.103924   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.903625   0.845633   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.890064   0.830870   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.772138   0.636360   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.356061   1.476266   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.356061   1.476266   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.356061   1.476266   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.356061   1.476266   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.356061   1.476266   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.356061   1.476266   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.113682   0.187889   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.094437   0.167958   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.574491  -0.615529   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    1.090329   0.913385   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.897677   0.729241   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -5.912393  -6.323709   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.339534   0.532214   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.292303   0.235531   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.438889   0.258486   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.438889   0.258486   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.208253   0.313572   \n",
       "top_tokens_coherence_other_implementation_score...    0.155120   0.213721   \n",
       "distance_to_nearest                                   0.917157   0.506945   \n",
       "\n",
       "                                                      topic_12    topic_13  \\\n",
       "kernel_size                                         121.000000  407.000000   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.116741    0.102491   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.583990    0.500487   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.113667    0.101379   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.100431    0.100097   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.492089    0.642143   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.479616    0.631021   \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.417972    0.574683   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.247272    1.112245   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.247272    1.112245   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.247272    1.112245   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.247272    1.112245   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.247272    1.112245   \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.247272    1.112245   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.121900    0.121448   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.102463    0.100667   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.620008   -0.690838   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.539923    0.890184   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.352384    0.690184   \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -6.598900   -6.158614   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.583990    0.432630   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.224648    0.221136   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.188524    0.100785   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.188524    0.100785   \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.287957    0.206843   \n",
       "top_tokens_coherence_other_implementation_score...    0.231643    0.104167   \n",
       "distance_to_nearest                                   0.958058    0.898429   \n",
       "\n",
       "                                                      topic_14  \n",
       "kernel_size                                         214.000000  \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.099711  \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.975982  \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.097300  \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.080213  \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.650835  \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.636409  \n",
       "intratext_coherence_score__tt_vw__cm_seg_weight...    0.525002  \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.442545  \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.442545  \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.442545  \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.442545  \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.442545  \n",
       "intratext_coherence_score__tt_vw__cm_seg_length...    1.442545  \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.123747  \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.104875  \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -0.588022  \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.861727  \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...    0.664889  \n",
       "intratext_coherence_score__tt_vw__cm_sow__wtrt_...   -6.135952  \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.894360  \n",
       "top_tokens_coherence_score__tt_vw__wtrt_pwt__se...    0.689570  \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.602910  \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.602910  \n",
       "top_tokens_coherence_score__tt_vw__wtrt_ptw__se...    0.655232  \n",
       "top_tokens_coherence_other_implementation_score...    0.143040  \n",
       "distance_to_nearest                                   0.533940  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer._topic_bank.view_topic_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are also saved (topics as $\\Phi$ matrices and topic score values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0__phi.bin\t   model_4__phi.bin\t      model_8__phi.bin\r\n",
      "model_0__topic_scores.bin  model_4__topic_scores.bin  model_8__topic_scores.bin\r\n",
      "model_1__phi.bin\t   model_5__phi.bin\t      model_9__phi.bin\r\n",
      "model_1__topic_scores.bin  model_5__topic_scores.bin  model_9__topic_scores.bin\r\n",
      "model_2__phi.bin\t   model_6__phi.bin\t      topics.bin\r\n",
      "model_2__topic_scores.bin  model_6__topic_scores.bin  topic_scores.bin\r\n",
      "model_3__phi.bin\t   model_7__phi.bin\r\n",
      "model_3__topic_scores.bin  model_7__topic_scores.bin\r\n"
     ]
    }
   ],
   "source": [
    "! ls $optimizer._topic_bank._path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing<a id=\"postprocessing\"></a>\n",
    "\n",
    "<div style=\"text-align: right\">Back to <a href=#contents>Contents</a></div>\n",
    "\n",
    "Remember, we were going to do computations several times (with different seeds and documents used by coherence scores).\n",
    "Here we combine these results to get just one topic bank.\n",
    "Supposedly, topic banks for different seeds should *not* differ much: exactly the same models are used in all cases.\n",
    "What is different is just the way we estimate topics quality (or, better, the documents we use for topic coherence computation).\n",
    "\n",
    "P.S.\n",
    "As I am writing the explanatory .md text, I realize that bank creation process could have been optimized: models should have been trained *only once* for each dataset.\n",
    "And then, for different seeds, would go quality estimation without actual training.\n",
    "In the version presented in the notebook there are model training for each seed.\n",
    "However, coherence computation still remains the most time consuming part (and this one we should be done separately for each seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vw.txt',\n",
       " 'vocab.txt',\n",
       " 'dict.dict.txt',\n",
       " 'new_ppmi_tf_',\n",
       " 'ppmi_tf_',\n",
       " 'dict.dict',\n",
       " 'cooc_values.json',\n",
       " 'batches',\n",
       " 'result']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATASET_INTERNALS_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals/result'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_RESULTS_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bank__11221963',\n",
       " 'search_result__11221963.json',\n",
       " 'bank__0',\n",
       " 'search_result__0.json',\n",
       " 'search_result__42.json',\n",
       " 'bank__42']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(SEARCH_RESULTS_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking what is insides one bank's folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_0__phi.bin', 'model_0__theta.bin', 'model_0__topic_scores.bin', 'model_10__phi.bin', 'model_10__theta.bin', 'model_10__topic_scores.bin', 'model_11__phi.bin', 'model_11__theta.bin', 'model_11__topic_scores.bin', 'model_12__phi.bin']\n",
      "...\n",
      "['model_7__theta.bin', 'model_7__topic_scores.bin', 'model_8__phi.bin', 'model_8__theta.bin', 'model_8__topic_scores.bin', 'model_9__phi.bin', 'model_9__theta.bin', 'model_9__topic_scores.bin', 'topic_scores.bin', 'topics.bin']\n"
     ]
    }
   ],
   "source": [
    "one_dataset_bank_folder_path = os.path.join(\n",
    "    SEARCH_RESULTS_FOLDER_PATH,\n",
    "    'bank__11221963',\n",
    ")\n",
    "\n",
    "folder_contents = sorted(os.listdir(one_dataset_bank_folder_path))\n",
    "\n",
    "print(folder_contents[:10])\n",
    "print('...')\n",
    "print(folder_contents[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What info is in one result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['optimum', 'optimum_std', 'bank_scores', 'bank_topic_scores', 'model_scores', 'model_topic_scores', 'num_bank_topics', 'num_model_topics'])\n"
     ]
    }
   ],
   "source": [
    "one_dataset_search_result_path = os.path.join(\n",
    "    SEARCH_RESULTS_FOLDER_PATH,\n",
    "    'search_result__11221963.json',\n",
    ")\n",
    "\n",
    "one_search_result = json.loads(open(one_dataset_search_result_path, 'r').read())\n",
    "\n",
    "print(one_search_result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What result fields we are interested in (for combining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResultKey(Enum):\n",
    "    BANK_SCORES = 'bank_scores'\n",
    "    BANK_TOPIC_SCORES = 'bank_topic_scores'\n",
    "    MODEL_SCORES = 'model_scores'\n",
    "    MODEL_TOPIC_SCORES = 'model_topic_scores'\n",
    "    NUM_BANK_TOPICS = 'num_bank_topics'\n",
    "    NUM_MODEL_TOPICS = 'num_model_topics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many items are there in each result field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank_scores\n",
      "    20 items\n",
      "bank_topic_scores\n",
      "    15 items\n",
      "model_scores\n",
      "    20 items\n",
      "model_topic_scores\n",
      "    20 items\n",
      "num_bank_topics\n",
      "    20 items\n",
      "num_model_topics\n",
      "    20 items\n"
     ]
    }
   ],
   "source": [
    "for search_result_key in SearchResultKey:\n",
    "    print(search_result_key.value)\n",
    "    print(' ' * 4 + str(len(one_search_result[search_result_key.value])) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of items in result fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank_scores\n",
      "    20 items\n",
      "    sample item: {perplexity_score: 1150.075439453125, sparsity_phi_score: 0.5031424164772034, sparsity_theta_score: 0.005107370670884848, ...}\n",
      "\n",
      "bank_topic_scores\n",
      "    15 items\n",
      "    sample item: {kernel_size: 229, intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_pwt__sem_none: 0.1296872250691411, top_tokens_coherence_score__tt_vw__wtrt_pwt__sem_none: 1.133653577000209, ...}\n",
      "\n",
      "model_scores\n",
      "    20 items\n",
      "    sample item: {perplexity_score: 739.5682373046875, sparsity_phi_score: 0.48647573590278625, sparsity_theta_score: 0.009358677081763744, ...}\n"
     ]
    }
   ],
   "source": [
    "for i, search_result_key in enumerate([\n",
    "        SearchResultKey.BANK_SCORES,\n",
    "        SearchResultKey.BANK_TOPIC_SCORES,\n",
    "        SearchResultKey.MODEL_SCORES]):\n",
    "\n",
    "    if i > 0:\n",
    "        print()\n",
    "\n",
    "    print(search_result_key.value)\n",
    "    print(' ' * 4 + str(len(one_search_result[search_result_key.value])) + ' items')\n",
    "    print(' ' * 4 + 'sample item: ', end='')\n",
    "    \n",
    "    one_item = next(iter(one_search_result[search_result_key.value]))\n",
    "    \n",
    "    print(\n",
    "        '{'\n",
    "        + ', '.join(\n",
    "            [f'{k}: {v}' for k, v in list(one_item.items())[:3]]\n",
    "        )\n",
    "        + ', ...'\n",
    "        + '}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_topic_scores\n",
      "    20 items\n",
      "    Item length: 100\n",
      "    Sample item: [{kernel_size: 397, intratext_coherence_score__tt_vw__cm_seg_weight__wtrt_pwt__sem_none: 0.017755509703420103, top_tokens_coherence_score__tt_vw__wtrt_pwt__sem_none: 0.3707505407301823, ...}, ...]\n"
     ]
    }
   ],
   "source": [
    "search_result_key = SearchResultKey.MODEL_TOPIC_SCORES\n",
    "\n",
    "print(search_result_key.value)\n",
    "print(' ' * 4 + str(len(one_search_result[search_result_key.value])) + ' items')\n",
    "\n",
    "one_item = next(iter(one_search_result[search_result_key.value]))\n",
    "\n",
    "print(' ' * 4 + f'Item length: {len(one_item)}')\n",
    "print(' ' * 4 + 'Sample item: ', end='')\n",
    "\n",
    "print(\n",
    "    '['\n",
    "    + '{'\n",
    "    + ', '.join(\n",
    "        [f'{k}: {v}' for k, v in list(next(iter(one_item)).items())[:3]]\n",
    "    )\n",
    "    + ', ...'\n",
    "    + '}'\n",
    "    + ', ...'\n",
    "    + ']'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bank_topics\n",
      "    Length: 20\n",
      "    [10, 10, 11, 11, 11, ..., 15, 15, 15, 15, 15]\n",
      "\n",
      "num_model_topics\n",
      "    Length: 20\n",
      "    [100, 100, 100, 100, 100, ..., 100, 100, 100, 100, 100]\n"
     ]
    }
   ],
   "source": [
    "for i, search_result_key in enumerate([\n",
    "        SearchResultKey.NUM_BANK_TOPICS,\n",
    "        SearchResultKey.NUM_MODEL_TOPICS]):\n",
    "\n",
    "    if i > 0:\n",
    "        print()\n",
    "\n",
    "    print(search_result_key.value)\n",
    "    \n",
    "    result_value = one_search_result[search_result_key.value]\n",
    "    \n",
    "    print(' ' * 4 + f'Length: {len(result_value)}')\n",
    "    print(\n",
    "        ' ' * 4\n",
    "        + '['\n",
    "        + ', '.join(str(n) for n in result_value[:5])\n",
    "        + ', ..., '\n",
    "        + ', '.join(str(n) for n in result_value[-5:])\n",
    "        + ']'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading banks corresponding to seeds (if there is a bank for such seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_BANK_SEEDS = [11221963, 42, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_TO_BANK = dict()\n",
    "\n",
    "for seed in POSSIBLE_BANK_SEEDS:\n",
    "    one_dataset_bank_folder_path = os.path.join(\n",
    "        SEARCH_RESULTS_FOLDER_PATH,\n",
    "        f'bank__{seed}',\n",
    "    )\n",
    "    \n",
    "    if not os.path.isdir(one_dataset_bank_folder_path):\n",
    "        print(f'No bank for such seed: {seed}.'\n",
    "              f' Folder \"{one_dataset_bank_folder_path}\" doesn\\'t exist')\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    SEED_TO_BANK[seed] = TopicBank(\n",
    "        save=False,\n",
    "        save_folder_path=one_dataset_bank_folder_path,\n",
    "    )\n",
    "\n",
    "\n",
    "BANK_SEEDS = list(SEED_TO_BANK.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeds for which there is a bank saved on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11221963, 42, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BANK_SEEDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of topics in bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    next(iter(SEED_TO_BANK.values())).topics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also number of topics, but some items in this list are `None` (when a topic is excluded from bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    next(iter(SEED_TO_BANK.values()))._topics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 15)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(SEED_TO_BANK.values())).view_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@default_class  проблема        0.377909\n",
       "                вопрос          0.051589\n",
       "                решать          0.039914\n",
       "                трудность       0.028569\n",
       "                возникать       0.026642\n",
       "                сталкиваться    0.022061\n",
       "                другой          0.015737\n",
       "                сложный         0.013366\n",
       "                создавать       0.011581\n",
       "                существовать    0.010672\n",
       "Name: topic_0, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED_TO_BANK[BANK_SEEDS[0]].view_topics()['topic_0'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@default_class  слово          0.193740\n",
       "                словарь        0.035589\n",
       "                буква          0.027122\n",
       "                значение       0.022216\n",
       "                речь           0.020765\n",
       "                глагол         0.016013\n",
       "                русский        0.012727\n",
       "                конструкция    0.010727\n",
       "                часто          0.009467\n",
       "                пример         0.008037\n",
       "Name: topic_0, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED_TO_BANK[BANK_SEEDS[1]].view_topics()['topic_0'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare banks corresponding to different seeds to see if they differ much or not (supposedly not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topics(\n",
    "        bank_seed1: int,\n",
    "        bank_seed2: int,\n",
    "        max_num_twins_to_display: int = 20) -> None:\n",
    "\n",
    "    topics_with_twin = set()\n",
    "    \n",
    "    print(\n",
    "        f'Close topics:'\n",
    "        f' <topic from \"{bank_seed1}\" bank>'\n",
    "        f' <topic from \"{bank_seed1}\" bank>'\n",
    "        f' <distance>'\n",
    "    )\n",
    "    \n",
    "    num_displayed = 0\n",
    "    \n",
    "    for i1, t1 in enumerate(SEED_TO_BANK[bank_seed1].topics):\n",
    "        for i2, t2 in enumerate(SEED_TO_BANK[bank_seed2].topics):\n",
    "            d = TopicBankMethod._jaccard_distance(t1, t2)\n",
    "\n",
    "            if d < 0.5:\n",
    "                topics_with_twin.add(i1)\n",
    "                num_displayed = num_displayed + 1\n",
    "                \n",
    "                print(f'{i1}\\t{i2}\\t{d}')\n",
    "            \n",
    "            if num_displayed >= max_num_twins_to_display:\n",
    "                break\n",
    "\n",
    "        if num_displayed >= max_num_twins_to_display:\n",
    "            break\n",
    "\n",
    "    print()\n",
    "\n",
    "    max_seed_length = max([len(str(s)) for s in [bank_seed1, bank_seed2]])\n",
    "\n",
    "    for seed in [bank_seed1, bank_seed2]:\n",
    "        print(\n",
    "            f'Num topics in bank \"{seed:{max_seed_length}}\":'\n",
    "            f' {len(SEED_TO_BANK[seed].topics)}'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nNum topics in bank \"{bank_seed1}\" with twins in bank \"{bank_seed2}\":'\n",
    "        f' {len(topics_with_twin)}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close topics: <topic from \"11221963\" bank> <topic from \"11221963\" bank> <distance>\n",
      "1\t1\t7.030248077910528e-06\n",
      "2\t2\t5.364596949331002e-06\n",
      "3\t4\t6.971623467633137e-06\n",
      "4\t5\t9.489878975976751e-06\n",
      "5\t6\t3.474788023982711e-06\n",
      "7\t7\t5.1163178486079985e-06\n",
      "8\t9\t6.845627906426621e-06\n",
      "11\t14\t1.107408409306565e-06\n",
      "12\t15\t1.2886343224716157e-06\n",
      "13\t10\t0.46749039824435024\n",
      "\n",
      "Num topics in bank \"11221963\": 15\n",
      "Num topics in bank \"      42\": 16\n",
      "\n",
      "Num topics in bank \"11221963\" with twins in bank \"42\": 10\n"
     ]
    }
   ],
   "source": [
    "if len(BANK_SEEDS) >= 2:\n",
    "    compare_topics(BANK_SEEDS[0], BANK_SEEDS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close topics: <topic from \"42\" bank> <topic from \"42\" bank> <distance>\n",
      "0\t0\t2.1602862702918557e-05\n",
      "1\t1\t1.4857055578576528e-05\n",
      "2\t2\t9.737322482217259e-06\n",
      "4\t3\t1.0445701357442161e-05\n",
      "5\t4\t1.668583528036116e-05\n",
      "6\t5\t6.556589902784182e-06\n",
      "7\t7\t9.581083945442437e-06\n",
      "9\t8\t1.2234920047093922e-05\n",
      "13\t11\t1.5169697592520848e-06\n",
      "14\t13\t6.072632612319495e-07\n",
      "15\t14\t1.1275848412761746e-06\n",
      "\n",
      "Num topics in bank \"42\": 16\n",
      "Num topics in bank \" 0\": 15\n",
      "\n",
      "Num topics in bank \"42\" with twins in bank \"0\": 11\n"
     ]
    }
   ],
   "source": [
    "if len(BANK_SEEDS) >= 3:\n",
    "    compare_topics(BANK_SEEDS[1], BANK_SEEDS[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we combine banks to get just one bank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_BANK_TOPIC_INDICES = dict()\n",
    "\n",
    "assert len(BANK_SEEDS) >= 1\n",
    "\n",
    "COMBINED_BANK_TOPIC_INDICES[BANK_SEEDS[0]] = list(\n",
    "    range(len(SEED_TO_BANK[BANK_SEEDS[0]].topics))\n",
    ")\n",
    "\n",
    "\n",
    "def get_final_bank_topics() -> Iterable[dict]:  # a token in dict may be as just one word,\n",
    "                                                # or a tuple of words\n",
    "    for seed, indices in COMBINED_BANK_TOPIC_INDICES.items():\n",
    "        for i in indices:\n",
    "            yield SEED_TO_BANK[seed].topics[i]\n",
    "\n",
    "\n",
    "def get_final_bank_topic_scores() -> Iterable[Dict[str, float]]:\n",
    "    for seed, indices in COMBINED_BANK_TOPIC_INDICES.items():\n",
    "        for i in indices:\n",
    "            yield SEED_TO_BANK[seed].topic_scores[i]\n",
    "\n",
    "\n",
    "DISTANCE_THRESHOLD = 0.5  # if topics in banks are really similar, this will be enough\n",
    "\n",
    "if len(BANK_SEEDS) == 1:\n",
    "    pass\n",
    "else:\n",
    "    for seed in tqdm(BANK_SEEDS[1:], total=len(BANK_SEEDS) - 1, file=sys.stdout):\n",
    "        COMBINED_BANK_TOPIC_INDICES[seed] = list()\n",
    "\n",
    "        for i, t in enumerate(SEED_TO_BANK[seed].topics):\n",
    "            if any([TopicBankMethod._jaccard_distance(t, bank_topic) < DISTANCE_THRESHOLD\n",
    "                    for bank_topic in get_final_bank_topics()]):\n",
    "\n",
    "                continue\n",
    "\n",
    "            COMBINED_BANK_TOPIC_INDICES[seed].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      seed\tnum_topics\n",
      "------------------------------\n",
      "  11221963\t        15\n",
      "        42\t         6\n",
      "         0\t         3\n",
      "------------------------------\n",
      "          \t        24\n"
     ]
    }
   ],
   "source": [
    "print('{0:>10}\\t{1:>10}'.format('seed', 'num_topics'))\n",
    "print('-' * 30)\n",
    "\n",
    "for seed, topic_indices in COMBINED_BANK_TOPIC_INDICES.items():\n",
    "    print(f'{seed:10}\\t{len(topic_indices):10}')\n",
    "\n",
    "print('-' * 30)\n",
    "\n",
    "print(\n",
    "    ' ' * 10\n",
    "    + '\\t'\n",
    "    + f'{sum(len(inds) for inds in COMBINED_BANK_TOPIC_INDICES.values()):10}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Output of the previous cell for all datasets:\n",
    "\n",
    "```\n",
    "PostNauka:\n",
    "\n",
    "seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        15\n",
    "        42\t         6\n",
    "         0\t         3\n",
    "------------------------------\n",
    "          \t        24\n",
    "\n",
    "Reuters:\n",
    "\n",
    "seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        22\n",
    "        42\t        10\n",
    "         0\t         1\n",
    "------------------------------\n",
    "          \t        33\n",
    "\n",
    "Brown:\n",
    "\n",
    "seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        23\n",
    "        42\t        16\n",
    "         0\t         8\n",
    "------------------------------\n",
    "          \t        47\n",
    "\n",
    "20 NG:\n",
    "\n",
    "seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        27\n",
    "        42\t        13\n",
    "         0\t        15\n",
    "------------------------------\n",
    "          \t        55\n",
    "\n",
    "AG News:\n",
    "\n",
    "seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        37\n",
    "        42\t         7\n",
    "         0\t         6\n",
    "------------------------------\n",
    "          \t        50\n",
    "\n",
    "Watan:\n",
    "\n",
    "     seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        10\n",
    "        42\t         5\n",
    "         0\t         1\n",
    "------------------------------\n",
    "          \t        16\n",
    "\n",
    "Habrahabr:\n",
    "\n",
    "seed\tnum_topics\n",
    "------------------------------\n",
    "  11221963\t        11\n",
    "        42\t        11\n",
    "------------------------------\n",
    "          \t        22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_SEARCH_RESULT_FOLDER_PATH = os.path.join(\n",
    "    DATASET_INTERNALS_FOLDER_PATH,\n",
    "    'result_combined',\n",
    ")\n",
    "\n",
    "COMBINED_BANK_FOLDER_PATH = os.path.join(\n",
    "    COMBINED_SEARCH_RESULT_FOLDER_PATH,\n",
    "    'bank',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/result_combined'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMBINED_SEARCH_RESULT_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Post_Science__internals__test/result_combined/bank'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMBINED_BANK_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(COMBINED_SEARCH_RESULT_FOLDER_PATH, exist_ok=True)\n",
    "os.makedirs(COMBINED_BANK_FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_BANK = TopicBank(\n",
    "    save=False,\n",
    "    save_folder_path=COMBINED_BANK_FOLDER_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_BANK._topics = list(get_final_bank_topics())\n",
    "COMBINED_BANK._topic_scores = list(get_final_bank_topic_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if all OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(COMBINED_BANK._topics) == len(COMBINED_BANK._topic_scores)\n",
    "assert len(COMBINED_BANK._topics) == sum(\n",
    "    len(v) for v in COMBINED_BANK_TOPIC_INDICES.values()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(COMBINED_BANK_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_BANK.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_scores.bin', 'topics.bin']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(COMBINED_BANK_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del COMBINED_BANK\n",
    "del COMBINED_BANK_TOPIC_INDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now topic banks are ready (all info is in `COMBINED_BANK_FOLDER_PATH` for each dataset).\n",
    "Notebook [TopicBank-Experiment: Model Validation](TopicBank-Experiment-ModelValidation.ipynb) goes next: we are going to estimate topic models quality with the help of topic bank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topnum",
   "language": "python",
   "name": "topnum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
